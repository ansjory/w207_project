{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project: House Prices Prediction\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques  \n",
    "\n",
    "### Team MLAIRE: Chi Iong Ansjory, Prabhat Tripathi, Soodong Kim, Tina Agarwal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperlinks directly to subsections\n",
    "- [1. Problem](#problem)\n",
    "- [2. Import Libraries](#import_libraries)\n",
    "- [3. Data Loading](#data_loading)\n",
    "- [4. Exploratory Data Analysis](#eda)\n",
    "- [5. Feature Engineering](#feature_engineering)\n",
    "- [6. Modeling](#modeling)\n",
    "- [7. Summary & Key Results](#summary)\n",
    "- [8. References](#references)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='problem'></a>\n",
    "# 1. Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to predict sale price of residential homes in Ames, Iowa based on the given training and test data sets. There are 79 explanatory variables describing different aspects of residential homes with 1,460 observations in both training and test data sets. \n",
    "\n",
    "The output variable (*SalePrice*) is numeric (interval) whereas the explanatory (LHS) variables are a combination of numeric (interval) and categorical variables. \n",
    "\n",
    "Considering the size and nature of the data set, and informed by our domain understanding, we believe that *feature engineering* -- crafting data features optimized for machine learning -- is key for better modeling for this problem. Specifically, we plan to perform following:\n",
    "\n",
    "1. Load data, perform sanity check and fix any obvious data errors.\n",
    "\n",
    "\n",
    "2. Perform exploratory data analysis (EDA) on the train data. This includes univariate and bi-variate analysis of  explanatory features to understand their relationships with the output variable *SalePrice*.\n",
    "\n",
    "\n",
    "3. Feature Engineering\n",
    "   - Missing values and their appropriate imputation.\n",
    "   - Transformation of features and feature scaling, if needed.\n",
    "   - Encodings to convert categorical features into numerical features so that we can run regression models.\n",
    "   - Split train and dev data sets.\n",
    "   \n",
    "\n",
    "4. Modeling\n",
    "   - Here we will try running several regression models and compare them for their prediction results. We will choose the best model or an ensamble of candidate models for final prediction. We also have to watch out for overfitting considering there are a large number of input variables (compared to the training data size).\n",
    "\n",
    "\n",
    "5. Summarize the key findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='import_libraries'></a>\n",
    "# 2. Import Libraries\n",
    "\n",
    "We import necessary libraries such as sklearn, scipy, pandas, numpy, and seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Each plot will not open a new window. \n",
    "# required libraries\n",
    "## pandas\n",
    "## seaborn\n",
    "%matplotlib inline\n",
    "\n",
    "# Import relevant libraries.\n",
    "\n",
    "# General libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# SK-learn libraries for Projection/learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Sk-Learn libraries for data mangling.\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "\n",
    "# Libraries for files\n",
    "import os\n",
    "\n",
    "# Python fundamental libraries\n",
    "import collections\n",
    "\n",
    "# Disable warnings for more clear output\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_loading'></a>\n",
    "# 3. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "\n",
    "We import both train and test data from csv files, make copies of both, and display the origin shapes of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shapes of train and test datasets:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1460, 81), (1459, 80))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Please keep this relative path access to data.\n",
    "dir_name = os.getcwd()\n",
    "train_filename = os.path.join(dir_name, 'data/train.csv')\n",
    "test_filename = os.path.join(dir_name, 'data/test.csv')\n",
    "\n",
    "# Reading the data\n",
    "train = pd.read_csv(train_filename)\n",
    "test  = pd.read_csv(test_filename)\n",
    "\n",
    "# We copy and preserve the original data frames for reference\n",
    "train_orig = train.copy()\n",
    "test_orig = test.copy()\n",
    "\n",
    "print(\"Original shapes of train and test datasets:\")\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "# 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following hyperlinks direct to relevant subsections:\n",
    "- [Feature Overview](#feature_overview)\n",
    "- [Numeric Features](#numeric_features)\n",
    "- [Categorical Features](#categorical_features)\n",
    "- [Feature Summary](#features_summary)\n",
    "- [Univariate Analysis](#univariate_analysis)\n",
    "    - [SalePrice](#saleprice)\n",
    "    - [GrLivArea](#grlivarea)\n",
    "    - [LotArea](#lotarea)\n",
    "    - [PoolArea](#poolarea)\n",
    "    - [OverallQual](#overallqual)\n",
    "    - [YearBuilt](#yearbuilt)\n",
    "- [Bivariate Analysis](#bivariate_analysis)\n",
    "    - [SalePrice - GrLivArea Relationship](#saleprice_grlivarea)\n",
    "    - [SalePrice - OverallQual Relationship](#saleprice_overallqual)\n",
    "    - [SalePrice - YearBuilt Relationship](#saleprice_yearbuilt)\n",
    "    - [SalePrice - GarageCars Relationship](#saleprice_garagecars)\n",
    "    - [SalePrice - Picked Variables Relationship](#saleprice_pickedvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a glimpse of the data set\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feature_overview'></a>\n",
    "## Feature Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin with discriminating between numeric and categorical features (including SalePrice output variable)\n",
    "numeric_columns = [f for f in train.columns if train.dtypes[f] != 'object']\n",
    "category_columns = [f for f in train.columns if train.dtypes[f] == 'object']\n",
    "print('Number of numeric features: {}'.format(len(numeric_columns)))\n",
    "print('Number of categorical features: {}'.format(len(category_columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='numeric_features'></a>\n",
    "## Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From data exploration, we observed that 3 numeric features do not represent interval data. In fact, they should\n",
    "# be treated as categorical features. Convert 3 numeric features to categorical by changing their type.\n",
    "train['MoSold'] = train.astype(str)\n",
    "train['YrSold'] = train.astype(str)\n",
    "train['MSSubClass'] = train.astype(str)\n",
    "\n",
    "# Re-identifying numeric features\n",
    "numeric_columns = [f for f in train.columns if train.dtypes[f] != 'object']\n",
    "print('* Number of numeric features: {}'.format(len(numeric_columns)))\n",
    "\n",
    "# Print numeric features and some properties\n",
    "print(\"------------------------------------- Numeric Features --------------------------------------------\")\n",
    "print(numeric_columns)\n",
    "print(\"---------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "# We also look at numeric features that have small number of unique \"discrete\" values\n",
    "discrete_columns = []\n",
    "print(\"\\n* Discrete features (Numeric features with small number of unique values)\")\n",
    "for col in numeric_columns:\n",
    "    if (train[col].nunique() <= 10):\n",
    "        discrete_columns.append(col)\n",
    "        print ('{}: {}'.format(col, train[col].nunique()))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='categorical_features'></a>\n",
    "## Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_columns = [f for f in train.columns if train.dtypes[f] == 'object']\n",
    "print('* Number of categorical features: {}'.format(len(category_columns)))\n",
    "# print Categorical feature names\n",
    "print(\"----------------------------------- Categorical Features ------------------------------------------\")\n",
    "print(category_columns)\n",
    "print(\"---------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "# For ordinal variables, from data description and exploration, we observed several categorical variables whose\n",
    "# values have order. This may be important in modeling where we treat these variables differently.\n",
    "ordinals_columns = ['KitchenQual', 'Fence', 'Functional', 'LotShape','LandSlope','ExterQual','ExterCond','BsmtQual',\n",
    "           'BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','HeatingQC','Electrical',\n",
    "            'FireplaceQu','GarageFinish','GarageQual','GarageCond','PavedDrive','PoolQC']\n",
    "\n",
    "print(\"\\n* Candidate Ordinal features (Categorical features with small number of unique values)\")\n",
    "for col in ordinals_columns:\n",
    "    if (train[col].nunique() <= 10):\n",
    "        print ('{}: {}, {}'.format(col, train[col].nunique(), train[col].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will understand the relationship of candidate ordinal features and SalePrice to determine if they are truly ordinal features.\n",
    "We will do this check by understanding the relationship between a feature and SalePrice. If SalePrice incrases with the \n",
    "order of values in the a feature, we can model the feature as Ordinal feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='features_summary'></a>\n",
    "## Feature Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above analysis, we categorized features into 4 categories. Numeric, Numeric (Discrete), Categorical, Catagorical (Ordinal).  \n",
    "\n",
    "The following table summarizes all the features by including important properties for each of them.  \n",
    "\n",
    "Descriptions of variables are based on https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data. Variable types are either categorical, or discrete/non-discrete numeric. There are 4 different \"Segments\": Sale (sale transaction of house), Location (where the house located), Building (physical characteristics), Space (space properties of house). For numerical variables, mean, min, max, and number of NaNs are derived from descriptive statistics, and also determine if it is discrete from the histogram. For categorical variables, number of NaNs and distinct values are determined by histogram. There are 3 levels of \"Expectations\": High, Medium, and Low of how the variables are related to the sale price. Column \"Selected\" indicates if the variables are included in the model based on the following analysis.\n",
    "\n",
    "|  | Variable | Description | Type | Segment | Mean | Min | Max | NaN | Distinct Value | Expectated Relevance | Comments |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 1 | SalePrice | Property's sale price in dollars | Numeric | Building  | 180921 | 34900 | 755000 | 0 | | | Output variable |  |\n",
    "| 2 | MSSubClass | Building class | Categorical| Building  | | | | 0 | 15 | Low |  |\n",
    "| 3 | MSZoning | General zoning classification | Categorical | Building | | | | 0 | 5  | Low |  |\n",
    "| 4 | LotFrontage | Linear feet of street connected to property | Numeric | Space |70 | 21 | 313 | 259 | | Medium |  |\n",
    "| 5 | LotArea | Lot size in square feet | Numeric | Space | 10516 | 1300 | 215245 | 0 | | High |  |\n",
    "| 6 | Street | Type of road access | Categorical | Location | | | | 0 | 2 | Low |  |\n",
    "| 7 | Alley | Type of alley access | Categorical | Location | | | | | 3 | Low | Most values NAs. |\n",
    "| 8 | LotShape | General shape of property |Categorical (Ordinal) | Location |  | | | 0 | 4 | Low |  |\n",
    "| 9 | LandContour | Flatness of the property | Categorical | Location |  | | | 0 | 9 | Low |  |\n",
    "| 10 | Utilities | Type of utilities available |Categorical |  Location |  | | | 0 | 1 | Low | Almost all values same (AllPub) |\n",
    "| 11 | LotConfig | Lot configuration |Categorical | Location |  | | | 0 | 5 | Medium |  |\n",
    "| 12 | LandSlope | Slope of property | Categorical (Ordinal)  | Location | | | | 0 | 3 | Low |  |\n",
    "| 13 | Neighborhood | Physical locations within Ames city limits | Categorical | Location  |  | | | 0 | 25 | Medium |  |\n",
    "| 14 | Condition1 | Proximity to main road or railroad |Categorical | Location | | | | 0 | 9 | Medium |  |\n",
    "| 15 | Condition2 | Proximity to main road or railroad (if a second is present) | Categorical | Location | | | | 0 | 2| Low |  |\n",
    "| 16 | BldgType | Type of dwelling | Categorical | Building | | | | 0 | 5 | Low |  |\n",
    "| 17 | HouseStyle | Style of dwelling |Categorical | Building |  | | | 0 | 8 | Low |  |\n",
    "| 18 | OverallQual | Overall material and finish quality | Numeric (Discrete) | Building |6 | 1 | 10 | 0 | | High |  |\n",
    "| 19 | OverallCond | Overall condition rating | Numeric (Discrete)  | Building | 5 | 1 | 9 | 0 | | Medium |  |\n",
    "| 20 | YearBuilt | Original construction date | Categorical |Building  | | | | 0 | 112 | High |  |\n",
    "| 21 | YearRemodAdd | Remodel date | Categorical | Building  | | | | 0 | 61 | High |  |\n",
    "| 22 | RoofStyle | Type of roof | Categorical | Building | | | | 0 | 5 | Medium |  |\n",
    "| 23 | RoofMatl | Roof material | Categorical | Building | | | | 0 | 4 | Low |  |\n",
    "| 24 | Exterior1st | Exterior covering on house | Categorical | Building | | | | 0 | 12 | Medium |  |\n",
    "| 25 | Exterior2nd | Exterior covering on house (if more than one material) | Categorical | Building | | | | 0 | 16| Low |  |\n",
    "| 26 | MasVnrType | Masonry veneer type | Categorical | Building  |  | | | 0 | 4 | Low |  |\n",
    "| 27 | MasVnrArea | Masonry veneer area in square feet | Numeric| Space  | 103 | 0 | 1600 | 8 | 0 | Low |  |\n",
    "| 28 | ExterQual | Exterior material quality | Categorical (Ordinal) | Building | | | | 0 | 4 | Low |  |\n",
    "| 29 | ExterCond | Present condition of the material on the exterior | Categorical (Ordinal) | Building  | | | | 0 | 3 | Medium  |  |\n",
    "| 30 | Foundation | Type of foundation | Categorical | Building | | | | 0 | 6 | Low | |\n",
    "| 31 | BsmtQual | Evaluates the height of the basement | Categorical (Ordinal) | Building | | | | 0 | 4 | Low |  |\n",
    "| 32 | BsmtCond | General condition of the basement | Categorical (Ordinal) | Building | | | | 0 | 3 | Medium |  |\n",
    "| 33 | BsmtExposure | Walkout or garden level basement walls | Categorical (Ordinal)  | Building | | | | 0 | 4 | Low |  |\n",
    "| 34 | BsmtFinType1 | Quality of basement finished area | Categorical (Ordinal)  | Building  | | | | 0 | 6 | Low |  |\n",
    "| 35 | BsmtFinSF1 | Type 1 finished square feet | Numeric | Space |443 | 0 | 5644 | 0 | | Medium |  |\n",
    "| 36 | BsmtFinType2 | Quality of second finished area (if present) | Categorical (Ordinal)  | Building | | | | 0 | 6 | Low |  |\n",
    "| 37 | BsmtFinSF2 | Type 2 finished square feet | Numeric | Space | 46 | 0 | 1474 | 0 | | Low |  |\n",
    "| 38 | BsmtUnfSF | Unfinished square feet of basement area | Numeric | Space | 567 | 0 | 2336 | 0 | | Medium |  |\n",
    "| 39 | TotalBsmtSF | Total square feet of basement area | Numeric | Space | 1057 | 0 | 6110 | 0 | | High |  |\n",
    "| 40 | Heating | Type of heating | Categorical | Building  | | | | 0 | 3 | Low |  |\n",
    "| 41 | HeatingQC | Heating quality and condition | Categorical (Ordinal)  | Building | | | | 0 | 5 | Low |  |\n",
    "| 42 | CentralAir | Central air conditioning | Categorical | Building  | | | | 0 | 2 | Medium |  |\n",
    "| 43 | Electrical | Electrical system | Categorical (Ordinal)  | Building | | | | 1 | 5 | Low |  |\n",
    "| 44 | 1stFlrSF | First Floor square feet | Numeric | Space | 1162 | 334 | 4692 | 0 | | High |  |\n",
    "| 45 | 2ndFlrSF | Second floor square feet | Numeric | Space | 346 | 0 | 2065 | 0 | | Medium |  |\n",
    "| 46 | LowQualFinSF | Low quality finished square feet (all floors) | Numeric | Space | 5 | 0 | 572 | 0 | | Low |  |\n",
    "| 47 | GrLivArea | Above grade (ground) living area square feet | Numeric | Space | 1515 | 334 | 5642 | 0 | | High |  |\n",
    "| 48 | BsmtFullBath | Basement full bathrooms | Numeric (Discrete) | Building | 0 | 0 | 3 | 0 | | Low |  |\n",
    "| 49 | BsmtHalfBath | Basement half bathrooms | Numeric (Discrete) | Building | 0 | 0 | 2 | 0 | | Low |  |\n",
    "| 50 | FullBath | Full bathrooms above grade | Numeric (Discrete) | Building | 1 | 0 | 3 | 0 | | High |  |\n",
    "| 51 | HalfBath | Half baths above grade | Numeric (Discrete) | Building |  0 | 0 | 2 | 0 | | Low |  |\n",
    "| 52 | BedroomAbvGr | Number of bedrooms above basement level | Numeric (Discrete) | Building | 2 | 0 | 8 | 0 | | Medium |  |\n",
    "| 53 | KitchenAbvGr | Number of kitchens | Numeric (Discrete) | Building | 1 | 0 | 3 | 0 | | Low |  |\n",
    "| 54 | KitchenQual | Kitchen quality | Categorical (Ordinal) | Building | | | | 0 | 4 | Medium |  |\n",
    "| 55 | TotRmsAbvGrd | Total rooms above grade (does not include bathrooms) | Numeric | Building  | 6 | 2 | 14 | 0 | | High |  |\n",
    "| 56 | Functional | Home functionality rating | Categorical (Ordinal) | Building | | | | 0 | 7 | Low |  |\n",
    "| 57 | Fireplaces | Number of fireplaces | Numeric (Discrete) | Building | 0 | 0 | 3 | 0 | | Medium |  |\n",
    "| 58 | FireplaceQu | Fireplace quality | Categorical (Ordinal)  | Building | | | | 690 | 5 | Low |  |\n",
    "| 59 | GarageType | Garage location | Categorical | Building | | | | 81 | 6 | Low |  |\n",
    "| 60 | GarageYrBlt | Year garage was built | Categorical | Building | | | | 81 | 97 | Low |  |\n",
    "| 61 | GarageFinish | Interior finish of the garage | Categorical (Ordinal)  | Building | | | | 81 | 3 | Low |  |\n",
    "| 62 | GarageCars | Size of garage in car capacity | Numeric (Discrete) | Building | 1 | 0 | 4 | 0 | | High |  |\n",
    "| 63 | GarageArea | Size of garage in square feet | Numeric | Space | 472 | 0 | 1418 | 0 | | High |  |\n",
    "| 64 | GarageQual | Garage quality | Categorical (Ordinal)  | Building | | | | 81 | 5 | Low |  |\n",
    "| 65 | GarageCond | Garage condition | Categorical (Ordinal)  | Building | | | | 81 | 5 | Low |  |\n",
    "| 66 | PavedDrive | Paved driveway | Categorical (Ordinal)  | Building | | | | 0 | 3 | Medium |  |\n",
    "| 67 | WoodDeckSF | Wood deck area in square feet | Numeric | Space | 94 | 0 | 857 | 0 | | Low |  |\n",
    "| 68 | OpenPorchSF | Open porch area in square feet | Numeric | Space | 46 | 0 | 547 | 0 | | Low |  |\n",
    "| 69 | EnclosedPorch | Enclosed porch area in square feet | Numeric | Space | 21 | 0 | 552 | 0 | | Low |  |\n",
    "| 70 | 3SsnPorch | Three season porch area in square feet | Numeric | Space | 3 | 0 | 508 | 0 | | Low |  |\n",
    "| 71 | ScreenPorch | Screen porch area in square feet | Numeric | Space | 15 | 0 | 480 | 0 | | Low |  |\n",
    "| 72 | PoolArea | Pool area in square feet | Numeric (Discrete) | Space | 2 | 0 | 738 | 0 | | Medium |  |\n",
    "| 73 | PoolQC | Pool quality | Categorical (Ordinal)  | Building | | | | 1453 | 3 | Low |  |\n",
    "| 74 | Fence | Fence quality | Categorical (Ordinal) | Building | | | | 1179 | 4 | Low |  |\n",
    "| 75 | MiscFeature | Miscellaneous feature not covered in other categories | Categorical | Building | | | | 1406 | 4 | Low |  |\n",
    "| 76 | MiscVal | $Value of miscellaneous feature | Numeric | Building | 43 | 0 | 15500 | 0 | | Low |  |\n",
    "| 77 | MoSold | Month Sold | Categorical | Sale | | | | 0 | 12 | Medium |  |\n",
    "| 78 | YrSold | Year Sold | Categorical | Sale | | | | 0 | 5 | Medium |  |\n",
    "| 79 | SaleType | Type of sale | Categorical | Sale  | | | | 0 | 9 | Medium |  |\n",
    "| 80 | SaleCondition | Condition of sale | Categorical | Sale | | | | 0 | 6 | Medium | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='univariate_analysis'></a>\n",
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will look at several key columns (variables) individually to understand them better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='saleprice'></a>\n",
    "### SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "train['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram and normal probability plot\n",
    "plt.subplots(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "g = sns.distplot(train['SalePrice'], fit=norm).set_title(\"Histogram (before)\");\n",
    "plt.subplot(1,2,2)   \n",
    "g = stats.probplot(train['SalePrice'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness:  1.8828757597682129\n",
      "Kurtosis:  6.536281860064529\n"
     ]
    }
   ],
   "source": [
    "# Skewness and Kurtosis\n",
    "print(\"Skewness: \", train['SalePrice'].skew())\n",
    "print(\"Kurtosis: \", train['SalePrice'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the target variable *SalePrice* has a right-skewed distribution. Log transformation appears to reduce the skewness. A normally distributed target variable helps in better modeling the relationship between target and independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying log transformation\n",
    "plt.subplots(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "g = sns.distplot(np.log(train['SalePrice']), fit=norm).set_title(\"Histogram (after)\");\n",
    "plt.subplot(1,2,2)   \n",
    "g = stats.probplot(np.log(train['SalePrice']), plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above log transformation of the target variable helps reduce skewed distribution and the new distribution looks closer to normal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='grlivarea'></a>\n",
    "### GrLivArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "train['GrLivArea'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at the histogram to understand the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram and normal probability plot\n",
    "plt.subplots(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "g = sns.distplot(train['GrLivArea'], fit=norm).set_title(\"Histogram\");\n",
    "plt.subplot(1,2,2)   \n",
    "g = stats.probplot(train['GrLivArea'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like log transformation is needed to reduce the skewness and improve normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lotarea'></a>\n",
    "### LotArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "train['LotArea'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at the histogram to understand the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram and normal probability plot\n",
    "plt.subplots(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "g = sns.distplot(train['LotArea'], fit=norm).set_title(\"Histogram\");\n",
    "plt.subplot(1,2,2)   \n",
    "g = stats.probplot(train['LotArea'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram demonstrates normal distribution but has a long tail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='poolarea'></a>\n",
    "### PoolArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "train['PoolArea'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram and normal probability plot\n",
    "plt.subplots(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "g = sns.distplot(train['PoolArea'], fit=norm).set_title(\"Histogram\");\n",
    "plt.subplot(1,2,2)   \n",
    "g = stats.probplot(train['PoolArea'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the probability plot, only a handful of properties have swimming pools but majority are without one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='overallqual'></a>\n",
    "### OverallQual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "train['OverallQual'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "sns.distplot(train['OverallQual']).set_title(\"Histogram\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the discrete nature of *OverallQual* feature shows multi-model histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='yearbuilt'></a>\n",
    "### YearBuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "sns.distplot(train['YearBuilt']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted in descending order\n",
    "train['YearBuilt'].value_counts().plot(kind='bar', figsize=(20,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The year of house appears to follow power curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bivariate_analysis'></a>\n",
    "## Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we look at the relationship between target variable (*SalePrice*) and some other relevant features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix showing evidence of multicollearity\n",
    "# Remove Id column which is totally irrelevant\n",
    "train_ = train.drop(['Id'], axis=1, errors='raise')\n",
    "corrmat = train_.corr()\n",
    "\n",
    "# Saleprice correlation matrix\n",
    "k = 10 # number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "cm = np.corrcoef(train_[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f',\n",
    "                 annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()\n",
    "\n",
    "# Let us print top correlated features (both positively and negatively) with SalePrice\n",
    "corr_matrix = train_.corr()\n",
    "print (corr_matrix['SalePrice'].sort_values(ascending=False)[:10], '\\n') # top 10 values\n",
    "print ('----------------------')\n",
    "print (corr_matrix['SalePrice'].sort_values(ascending=False)[-10:]) # bottom 10 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the correlation matrix (lighter color -> better correlation), some of the highly correlated features appear to be *OverallQual*, *YearBuilt*, *YearRemodAdd*, *TotalBsmtSF*, *1stFlrSF*, *GrLivArea*, *FullBath*, *TotRmsAbvGrd*, *GarageCars*, and *GarageArea*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, some variables seem to be strongly correlated with the target variable. Here we see that the *OverallQual* feature is 79% correlated with the target variable. *OverallQual* feature Rates the overall material and finish of the house. This seems to be the parameter that affects the sale price positively. In addition, *GrLivArea* is 70% correlated with the target variable. *GrLivArea* refers to the living area (in sq ft.) above ground."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='saleprice_grlivarea'></a>\n",
    "### SalePrice - GrLivArea Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot of GrLivArea/SalePrice\n",
    "data = pd.concat([train['SalePrice'], train_['GrLivArea']], axis=1)\n",
    "data.plot.scatter(x='GrLivArea', y='SalePrice', ylim=(0,800000));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*GrLivArea* and *SalePrice* seem to have linear relationship of larger the area, higher the price. However, there are a few observable outliers, though, on the higher extremes of areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='saleprice_overallqual'></a>\n",
    "### SalePrice - OverallQual Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since *OverallQual* is a discrete variable, we look at the boxplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of OverallQual/SalePrice\n",
    "var = 'OverallQual'\n",
    "data = pd.concat([train['SalePrice'], train_[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8,6))\n",
    "fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n",
    "fig.axis(ymin=0, ymax=800000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot shows strong relationship between overall quality of the house and its sale price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='saleprice_yearbuilt'></a>\n",
    "### SalePrice - YearBuilt Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, boxplot also shows the general trend of higher price for newer house:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of YearBuilt/SalePrice\n",
    "var = 'YearBuilt'\n",
    "data = pd.concat([train['SalePrice'], train_[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(16,8))\n",
    "fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n",
    "fig.axis(ymin=0, ymax=800000)\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='saleprice_garagecars'></a>\n",
    "### SalePrice - GarageCars Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot shows that there is linear relationship between size of garage and price when garage size is between 0 and 3. House with 4-car garage, though, doesn't appears to positively influence sale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of GarageCars/SalePrice\n",
    "var = 'GarageCars'\n",
    "data = pd.concat([train['SalePrice'], train_[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8,6))\n",
    "fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n",
    "fig.axis(ymin=0, ymax=800000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='saleprice_pickedvars'></a>\n",
    "### SalePrice - Picked Variables Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatterplot of *SalePrice* vs the 10 picked variables, look like all demonstrate linear relationship with sale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot\n",
    "sns.set()\n",
    "cols = ['SalePrice', 'OverallQual', 'YearBuilt', 'YearRemodAdd', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea',\n",
    "        'FullBath', 'TotRmsAbvGrd', 'GarageCars', 'GarageArea']\n",
    "sns.pairplot(train_[cols], size=2.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feature_engineering'></a>\n",
    "# 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following hyperlinks direct to relevant subsections:\n",
    "- [Combine Train and Test Data](#combining_data)\n",
    "- [Remove Meaningless Columns](#remove_bad_columns)\n",
    "- [Missing Data Analysis](#missing_data_analysis)\n",
    "- [Missing Data Handling](#missing_data_handling)\n",
    "- [Missing Values Handling](#impute_NaN_values)\n",
    "- [Data Preparation](#data_preparation)\n",
    "    - [Log Transformations](#log_transformations)\n",
    "    - [Manually Encoding Ordinal Features](#ordinal_encoding)\n",
    "    - [Data Binning](#data_binning)\n",
    "    - [Cleanup For Binning Applied Columns](#binning_cleanup)\n",
    "    - [Categorical Features: Conversion to Dummy/Indicator Variables](#categorical_encoding)\n",
    "- [Split Back into Train and Test](#split_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(train[(train['GrLivArea'] > 4000)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='combining_data'></a>\n",
    "## Combine Train and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will combine train and test data frames so that all feature engineering changes are applied to both together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 81)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract and keep the SalePrice (target) column\n",
    "Y_train_all = train['SalePrice']\n",
    "# Drop SalePrice so that is has same cardinality as test dataframe\n",
    "train.drop('SalePrice', axis=1, inplace=True)\n",
    "\n",
    "# Preseve test dataset IDs. We will need them for final prediction/submission\n",
    "test_id = test.iloc[:,0]\n",
    "\n",
    "# Combine test and train\n",
    "combined_df = pd.concat([train, test]).reset_index()\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='remove_bad_columns'></a>\n",
    "## Remove Meaningless Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID is not useful\n",
    "combined_df.drop('Id', axis=1, inplace=True)\n",
    "\n",
    "# Almost all rows have same value for Utilities, making this variable not useful\n",
    "combined_df.drop('Utilities', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='missing_data_analysis'></a>\n",
    "## Missing Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will analyze missing values in the data set and approach to tackle these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Number of Missing Data</th>\n",
       "      <th>Missing Data Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PoolQC</th>\n",
       "      <td>2909</td>\n",
       "      <td>99.657417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MiscFeature</th>\n",
       "      <td>2814</td>\n",
       "      <td>96.402878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alley</th>\n",
       "      <td>2721</td>\n",
       "      <td>93.216855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fence</th>\n",
       "      <td>2348</td>\n",
       "      <td>80.438506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FireplaceQu</th>\n",
       "      <td>1420</td>\n",
       "      <td>48.646797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LotFrontage</th>\n",
       "      <td>486</td>\n",
       "      <td>16.649538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageCond</th>\n",
       "      <td>159</td>\n",
       "      <td>5.447071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageQual</th>\n",
       "      <td>159</td>\n",
       "      <td>5.447071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <td>159</td>\n",
       "      <td>5.447071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageFinish</th>\n",
       "      <td>159</td>\n",
       "      <td>5.447071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageType</th>\n",
       "      <td>157</td>\n",
       "      <td>5.378554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtCond</th>\n",
       "      <td>82</td>\n",
       "      <td>2.809181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtExposure</th>\n",
       "      <td>82</td>\n",
       "      <td>2.809181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtQual</th>\n",
       "      <td>81</td>\n",
       "      <td>2.774923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinType2</th>\n",
       "      <td>80</td>\n",
       "      <td>2.740665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <td>79</td>\n",
       "      <td>2.706406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MasVnrType</th>\n",
       "      <td>24</td>\n",
       "      <td>0.822199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MasVnrArea</th>\n",
       "      <td>23</td>\n",
       "      <td>0.787941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSZoning</th>\n",
       "      <td>4</td>\n",
       "      <td>0.137033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <td>2</td>\n",
       "      <td>0.068517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Total Number of Missing Data  Missing Data Percentage\n",
       "PoolQC                                2909                99.657417\n",
       "MiscFeature                           2814                96.402878\n",
       "Alley                                 2721                93.216855\n",
       "Fence                                 2348                80.438506\n",
       "FireplaceQu                           1420                48.646797\n",
       "LotFrontage                            486                16.649538\n",
       "GarageCond                             159                 5.447071\n",
       "GarageQual                             159                 5.447071\n",
       "GarageYrBlt                            159                 5.447071\n",
       "GarageFinish                           159                 5.447071\n",
       "GarageType                             157                 5.378554\n",
       "BsmtCond                                82                 2.809181\n",
       "BsmtExposure                            82                 2.809181\n",
       "BsmtQual                                81                 2.774923\n",
       "BsmtFinType2                            80                 2.740665\n",
       "BsmtFinType1                            79                 2.706406\n",
       "MasVnrType                              24                 0.822199\n",
       "MasVnrArea                              23                 0.787941\n",
       "MSZoning                                 4                 0.137033\n",
       "BsmtHalfBath                             2                 0.068517"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate about missing data\n",
    "# Analyze top 20 columns having missing data\n",
    "total = combined_df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (combined_df.isnull().sum() / combined_df.isnull().count() * 100).sort_values(ascending=False)\n",
    "missing_data = pd.concat(\n",
    "    [total, percent], axis=1, keys=[\"Total Number of Missing Data\", \"Missing Data Percentage\"])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='missing_data_handling'></a>\n",
    "## Missing Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Drop columns with ~80% or more missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_will_be_removed = ['PoolQC', 'MiscFeature', 'Alley', 'Fence']\n",
    "\n",
    "combined_df.drop(columns_will_be_removed, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "# Double check whether above columns are removed\n",
    "total = combined_df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (combined_df.isnull().sum() / combined_df.isnull().count() * 100).sort_values(ascending=False)\n",
    "\n",
    "def assert_column_drop(data, col_names):\n",
    "    for col_name in col_names:\n",
    "        assert col_name not in data, \"%s should not exist\" % col_name\n",
    "        \n",
    "assert_column_drop(combined_df, columns_will_be_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data set after removing columns with 80% missing data: (2919, 75)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of data set after removing columns with 80% missing data: {}\".format(combined_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='impute_NaN_values'></a>\n",
    "## Missing Values Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. From documentation: data_description.txt, the following column's default value (when NaN) is **None**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_none_columns = ['FireplaceQu', 'GarageType','GarageFinish','GarageQual',\n",
    "                        'GarageCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n",
    "                        'MasVnrType']\n",
    "\n",
    "for none_column in default_none_columns:\n",
    "    combined_df[none_column].fillna('None', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. From documentation: data_description.txt, the following column's default value (when NaN) is **0**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_zero_columns = ['GarageYrBlt','GarageArea','GarageCars','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n",
    "                        'TotalBsmtSF', 'BsmtFullBath','BsmtHalfBath','MasVnrArea']\n",
    "\n",
    "for zero_column in default_zero_columns:\n",
    "    combined_df[zero_column].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Using background knowledge, we set the missing value to the **mode** for some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Electrical'].fillna(combined_df['Electrical'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. LotFrontage: We will replace missing values with **mean** of the available data points for this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['LotFrontage'].fillna(combined_df['LotFrontage'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We observed that values for some rows in the **test** data set has missing values (these columns do not have missing values in the train dataset). Missing values for these Categorical (Ordinal) type features: we will impute them with **mode** of the available values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['MSZoning'].fillna(combined_df['MSZoning'].mode()[0], inplace=True)\n",
    "combined_df['Functional'].fillna(combined_df['Functional'].mode()[0], inplace=True)\n",
    "combined_df['SaleType'].fillna(combined_df['SaleType'].mode()[0], inplace=True)\n",
    "combined_df['KitchenQual'].fillna(combined_df['KitchenQual'].mode()[0], inplace=True)\n",
    "combined_df['Exterior1st'].fillna(combined_df['Exterior1st'].mode()[0], inplace=True)\n",
    "combined_df['Exterior2nd'].fillna(combined_df['Exterior2nd'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that there are no more missing values in the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert combined_df.isnull().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_preparation'></a>\n",
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will prepare data for machine learning modeling. This includes, preprocessing, transformations, encoding and scaling data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='log_transformations'></a>\n",
    "### Log Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As discussed in the EDA above, for SalePrice, we will perform log transformation\n",
    "Y_train_all = np.log(Y_train_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also perform log transformation for these numeric features. Note that since the min value of *PoolArea* is 0, we cannot perform log transformation. We will, instead, use *log1p* transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_features = ['LotArea','PoolArea']\n",
    "\n",
    "for log_feature in log_features:\n",
    "    combined_df[log_feature] = np.log1p(combined_df[log_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'Gd', 'Mn', 'Av', 'None'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df['BsmtExposure'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 2, 3, 0], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df['BsmtExposure'].map({\"None\":0, \"No\":1, \"Mn\":2, \"Av\":3, \"Gd\":4}).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ordinal_encoding'></a>\n",
    "### Manually Encoding Ordinal Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We manually encode ordered values for ordinal categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['BsmtQual'] = combined_df['BsmtQual'].map({\"None\":0, \"Fa\":1, \"TA\":2, \"Gd\":3, \"Ex\":4})\n",
    "combined_df['BsmtCond'] = combined_df['BsmtCond'].map({\"None\":0, \"Po\":1, \"Fa\":2, \"TA\":3, \"Gd\":4, \"Ex\":5})\n",
    "combined_df['BsmtExposure'] = combined_df['BsmtExposure'].map({\"None\" : 0, \"NA\":0, \"No\":1, \"Mn\":2, \"Av\":3, \"Gd\":4})\n",
    "combined_df['KitchenQual'] = combined_df['KitchenQual'].map({\"Fa\":1, \"TA\":2, \"Gd\":3, \"Ex\":4})\n",
    "combined_df['ExterQual'] = combined_df['ExterQual'].map({\"Po\" : 0, \"Fa\":1, \"TA\":2, \"Gd\":3, \"Ex\":4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['BsmtExposure'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_binning'></a>\n",
    "### Data Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To reduce side impact of filled data with default value, we decided to bin year data into pre-1950, 1950-1974, 1975 -2000, and post-2000, respectively. By this approach, if default value is filled by zero, it will be categorized into pre-1950 instead of recognizing data itself as zero. We can assume that if data is missing, house data may not be recorded properly since it was built or remodeled before 1950.\n",
    "2. After binning, the target column having continuous integer value will be dropped.\n",
    "3. As a final step, value will be transformed into discrete value such as 1, 2, 3, or 4. Since recent update on the property can have a positive impact, larger value assigned for the recent years can be justfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GarageBlt column values: 1,2,3,4\n",
      "YrRemodeled column values: 2,3,4\n",
      "YrBuilt column values: 1,2,4\n"
     ]
    }
   ],
   "source": [
    "PERIOD_TO_VALUE = {'0': 0, 'Before1950': 1, '1950to1974': 2, '1975to1999': 3, '2000OrLater': 4}\n",
    "\n",
    "# Data Binning\n",
    "\n",
    "# GarageYrBlt -> GarageBlt\n",
    "combined_df.loc[(combined_df[\"GarageYrBlt\"].apply(int) < 1950) & (combined_df[\"GarageYrBlt\"].apply(int) >= 0),\n",
    "                \"GarageBlt\"] = 'Before1950'\n",
    "combined_df.loc[(combined_df[\"GarageYrBlt\"].apply(int) < 1975) & (combined_df[\"GarageYrBlt\"].apply(int) >= 1950),\n",
    "                \"GarageBlt\"] = '1950to1974'\n",
    "combined_df.loc[(combined_df[\"GarageYrBlt\"].apply(int) < 2000) & (combined_df[\"GarageYrBlt\"].apply(int) >= 1975),\n",
    "                \"GarageBlt\"] = '1975to1999'\n",
    "combined_df.loc[combined_df[\"GarageYrBlt\"].apply(int) >= 2000, \"GarageBlt\"] = '2000OrLater'\n",
    "\n",
    "# YearRemodAdd -> YrRemodeled\n",
    "# Assume that there is no remodeling if built year == remodeled year\n",
    "combined_df.loc[combined_df[\"YearRemodAdd\"] == combined_df[\"YearBuilt\"], \"YrRemodeled\"] = 0\n",
    "combined_df.loc[(combined_df[\"YearRemodAdd\"] < 1950) & (combined_df[\"YearRemodAdd\"] != 0),\n",
    "                \"YrRemodeled\"] = 'Before1950'\n",
    "combined_df.loc[(combined_df[\"YearRemodAdd\"] < 1975) & (combined_df[\"YearRemodAdd\"] >= 1950),\n",
    "                \"YrRemodeled\"] = '1950to1974'\n",
    "combined_df.loc[(combined_df[\"YearRemodAdd\"] < 2000) & (combined_df[\"YearRemodAdd\"] >= 1975),\n",
    "                \"YrRemodeled\"] = '1975to1999'\n",
    "combined_df.loc[combined_df[\"YearRemodAdd\"] >= 2000, \"YrRemodeled\"] = '2000OrLater'\n",
    "\n",
    "# YearBuilt -> YrBuilt\n",
    "combined_df.loc[combined_df[\"YearBuilt\"] < 1950, \"YrBuilt\"] = 'Before1950'\n",
    "combined_df.loc[(combined_df[\"YearBuilt\"] < 1975) & (combined_df[\"YearBuilt\"] >= 1950), \"YrBuilt\"] = '1950to1974'\n",
    "combined_df.loc[(combined_df[\"YearBuilt\"] < 2000) & (combined_df[\"YearBuilt\"] >= 1975), \"YrBuilt\"] = '1950to1974'\n",
    "combined_df.loc[combined_df[\"YearBuilt\"] >= 2000, \"YrBuilt\"] = '2000OrLater'\n",
    "\n",
    "combined_df['GarageBlt'] = combined_df['GarageBlt'].apply(lambda period: PERIOD_TO_VALUE[period])\n",
    "combined_df['YrRemodeled'] = combined_df['YrRemodeled'].apply(lambda period: PERIOD_TO_VALUE[period])\n",
    "combined_df['YrBuilt'] = combined_df['YrBuilt'].apply(lambda period: PERIOD_TO_VALUE[period])\n",
    "\n",
    "def get_distinct_values(values):\n",
    "    return \",\".join([str(i) for i in (set(values))])\n",
    "\n",
    "# Check data binning results\n",
    "print(\"GarageBlt column values:\", get_distinct_values(combined_df[\"GarageBlt\"]))\n",
    "print(\"YrRemodeled column values:\", get_distinct_values(combined_df[\"YrRemodeled\"]))\n",
    "print(\"YrBuilt column values:\", get_distinct_values(combined_df[\"YrBuilt\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='binning_cleanup'></a>\n",
    "### Cleanup For Binning Applied Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns after data binning. Continuous data is no longer meaningful.\n",
    "combined_df.drop(['GarageYrBlt', 'YearRemodAdd', 'YearBuilt'], axis=1, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='categorical_encoding'></a>\n",
    "### Categorical Features: Conversion to Dummy/Indicator Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 266)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = pd.get_dummies(combined_df)\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='split_back'></a>\n",
    "## Split Back into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1460, 266), (1459, 266))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_all = combined_df[:train.shape[0]]\n",
    "X_test = combined_df[train.shape[0]:]\n",
    "X_train_all.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modeling'></a>\n",
    "# 6. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following hyperlinks direct to relevant subsections:\n",
    "- [Scaling Features](#scaled_feats)\n",
    "- [Create Cross Validation (Dev) Data Set for Testing Model Performance](#cross_validation)\n",
    "- [Using Grid Search to Tune Hyperparameters](#gridsearch_tuning)\n",
    "- [Predict Test and Prepare Submission](#predict_test_prep_submission)\n",
    "- [Models](#models)\n",
    "- [Partial Dependence](#partial_dependence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scaled_feats'></a>\n",
    "## Scaling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_all)\n",
    "\n",
    "X_train_all = scaler.transform(X_train_all)\n",
    "X_train_all = pd.DataFrame(X_train_all, columns = combined_df.columns)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns = combined_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scaled_feats'></a>\n",
    "## Important Features Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some resoning if we go this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# We use the base estimator LassoCV since the L1 norm promotes sparsity of features.\n",
    "clf = LassoCV(cv=5)\n",
    "\n",
    "# Set a minimum threshold of 0.25\n",
    "sfm = SelectFromModel(clf, threshold=0.01)\n",
    "sfm.fit(X_train_all, Y_train_all)\n",
    "n_features = sfm.transform(X_train_all).shape[1]\n",
    "print(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_selection import SelectFromModel\n",
    "#from sklearn.ensemble import GradientBoostingRegressor\n",
    "#gbModel = GradientBoostingRegressor()\n",
    "#gbModel.fit(X_train_all, Y_train_all)\n",
    "#gb_feat = SelectFromModel(gbModel, prefit = True)\n",
    "\n",
    "#X_train_all = gb_feat.transform(X_train_all)\n",
    "#X_test = gb_feat.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_all.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cross_validation'></a>\n",
    "## Create Cross Validation (Dev) Data Set for Testing Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cutoff to bisect train and cross validation data.\n",
    "cutoff = (len(train) * 80) // 100\n",
    "\n",
    "X_dev = X_train_all[cutoff:]\n",
    "Y_dev = Y_train_all[cutoff:]\n",
    "X_train = X_train_all[:cutoff]\n",
    "Y_train = Y_train_all[:cutoff]\n",
    "X_dev.shape,Y_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "res = defaultdict(dict)\n",
    "def benchmark(model, name=None):\n",
    "    if not name:\n",
    "        name = model.__class__.__name__\n",
    "    t0 = time.clock()\n",
    "    model.fit(X_train, Y_train)\n",
    "    res[name]['train_time'] = time.clock() - t0\n",
    "    res[name]['train_rmse'] = np.sqrt(mean_squared_error(Y_train, model.predict(X_train)))\n",
    "    \n",
    "    t0 = time.clock()\n",
    "    Y_pred = model.predict(X_dev)\n",
    "    res[name]['test_time'] = time.clock() - t0\n",
    "    res[name]['test_rmse'] = np.sqrt(mean_squared_error(Y_dev, Y_pred))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(LinearRegression())\n",
    "benchmark(Ridge(alpha=5.2))\n",
    "benchmark(ElasticNet(alpha=0.01))\n",
    "est = benchmark(GradientBoostingRegressor(learning_rate=0.1, n_estimators=200))\n",
    "benchmark(RandomForestRegressor(n_estimators=30, max_depth=10))\n",
    "res_df = pd.DataFrame(data=res).T\n",
    "res_df[['train_time', 'train_rmse', 'test_time', 'test_rmse']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gridsearch_tuning'></a>\n",
    "## Using Grid Search to Tune Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression\n",
    "alphas = np.array([10,5,0.1,0.01,0.001,0.0001,0])\n",
    "\n",
    "# Create and fit a ridge regression model, testing each alpha\n",
    "model = Ridge()\n",
    "grid = GridSearchCV(estimator=model, cv=10, param_grid=dict(alpha=alphas),scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid.fit(X_train_all, Y_train_all)\n",
    "\n",
    "# Print best params and score\n",
    "print('Best parameters:', grid.best_estimator_.alpha)\n",
    "print('Best score:', -grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso regression\n",
    "alphas = [0.00001, 0.0001, 0.001, 0.005, 0.01, 0.05]\n",
    "\n",
    "# Create and fit a ridge regression model, testing each alpha\n",
    "ls = Lasso()\n",
    "grid_ls = GridSearchCV(estimator=ls, param_grid=dict(alpha=alphas),scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_ls.fit(X_train_all, Y_train_all)\n",
    "\n",
    "# Print best params and score\n",
    "print('Best parameters:', grid_ls.best_estimator_.alpha)\n",
    "print('Best score:', -grid_ls.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic net regression\n",
    "param_grid = [{'alpha': np.logspace(0, 1, 10), 'l1_ratio': np.linspace(0, 1, 10)}]\n",
    "\n",
    "# Create and fit a Elastic net regression model\n",
    "grid_en = GridSearchCV(estimator=ElasticNet(), param_grid=param_grid,scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_en.fit(X_train_all, Y_train_all)\n",
    "\n",
    "# Print best params and score\n",
    "print('Best parameters: alpha: {}, l1_ratio: {}'.format(grid_en.best_estimator_.alpha,grid_en.best_estimator_.l1_ratio))\n",
    "print('Best score: {}'.format(-grid_en.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_en.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting model\n",
    "param_grid={'n_estimators':[100],\n",
    "           'learning_rate': [0.01,0.1, 1],\n",
    "           'max_depth':[4,6,10],\n",
    "           'min_samples_leaf':[3,5]\n",
    "           }\n",
    "gbModel = GridSearchCV(estimator=GradientBoostingRegressor(), cv=5, param_grid=param_grid, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "gbModel.fit(X_train_all, Y_train_all)\n",
    "print('Best parameters: n_estimators: {}, learning_rate: {}, max_depth: {}, min_samples_leaf: {}'.format(\n",
    "    gbModel.best_estimator_.n_estimators,gbModel.best_estimator_.learning_rate, gbModel.best_estimator_.max_depth, gbModel.best_estimator_.min_samples_leaf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbModel.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest model\n",
    "param_grid={'n_estimators':[50,100],\n",
    "           'max_depth':[5,10,20],\n",
    "           }\n",
    "rfModel = GridSearchCV(estimator=RandomForestRegressor(), cv=5, param_grid=param_grid, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "rfModel.fit(X_train_all, Y_train_all)\n",
    "print('Best estimators:', rfModel.best_estimator_.n_estimators)\n",
    "print('Best max depth:', rfModel.best_estimator_.max_depth)\n",
    "print('Best score:', rfModel.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfModel.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_ls.best_estimator_.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using best parameters from Grid Search CV to find the best model\n",
    "benchmark(LinearRegression())\n",
    "benchmark(Lasso(alpha=grid_ls.best_estimator_.alpha))\n",
    "benchmark(Ridge(alpha=grid.best_estimator_.alpha))\n",
    "benchmark(ElasticNet(alpha=grid_en.best_estimator_.alpha, l1_ratio=grid_en.best_estimator_.l1_ratio))\n",
    "est = benchmark(GradientBoostingRegressor(max_depth=gbModel.best_estimator_.max_depth, min_samples_leaf=gbModel.best_estimator_.min_samples_leaf, learning_rate=gbModel.best_estimator_.learning_rate, n_estimators=100))\n",
    "benchmark(RandomForestRegressor(n_estimators=rfModel.best_estimator_.n_estimators, max_depth=rfModel.best_estimator_.max_depth))\n",
    "res_df = pd.DataFrame(data=res).T\n",
    "res_df[['train_time', 'train_rmse', 'test_time', 'test_rmse']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing our best model GradientBoostingRegressor by getting relative importance of each attribute\n",
    "varImp = pd.DataFrame({'index':X_train_all.columns, 'feature_importance':gbModel.best_estimator_.feature_importances_})\n",
    "varImp.sort_values(by='feature_importance', ascending=False, inplace=True)\n",
    "f, ax = plt.subplots(1, 1, figsize=[12, 9])\n",
    "\n",
    "p = sns.barplot(x = 'feature_importance', y = 'index', data = varImp.iloc[:30,], ax = ax);\n",
    "p.set_ylabel(\"Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='predict_test_prep_submission'></a>\n",
    "## Predict Test and Prepare Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = GradientBoostingRegressor(max_depth=gbModel.best_estimator_.max_depth, min_samples_leaf=gbModel.best_estimator_.min_samples_leaf, learning_rate=gbModel.best_estimator_.learning_rate, n_estimators=100)\n",
    "final_model.fit(gb_train, Y_train_all)\n",
    "y_test = np.exp(final_model.predict(gb_test))\n",
    "pred_df = pd.DataFrame()\n",
    "pred_df['Id'] = test_id\n",
    "pred_df['SalePrice'] = y_test\n",
    "pred_df.to_csv('w207_mlairev2.csv',index=False)\n",
    "print('result csv generated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='models'></a>\n",
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we plan to perform:\n",
    "* Dimensionality reduction\n",
    "* Building and tuning home prices prediction models\n",
    "\n",
    "From 79 available features (input variables), we will use statistically and practically significant variables for modeling after data mangling. We also will try to watch out for collinearity and spurious relationships.  \n",
    "\n",
    "We plan to start with LinearRegression model because of the predictive nature of the problem. We will also try other supervised learning models such as Random Forest and Gradient Boosting Tree if they increase accuracy.\n",
    "\n",
    "We will be working on two broad sets of algorithms:\n",
    "1. Linear Models\n",
    "2. Non Linear relationships using Random Forests\n",
    "\n",
    "We plan to begin with (multiple) LinearRegression model (for speed) and if the accuracy is not satisfactory, we would try other models such as random forest and gradient-boosting tree. If needed, we may have to ensemble all these models for better overall accuracy. We will split test data into two random sets and use one as \"dev\" data during model building process.\n",
    "\n",
    "### Linear Models\n",
    "For linear models, we will try and test with the regular OLS model, and the regularized linear models of Ridge Regression, Least Absolute Shrinkage and Selection Operator (LASSO), and Elastic Net. \n",
    "\n",
    "For model tuning, Sklearn's grid search with CV function will be used to find the optimal hyper-parameter values.\n",
    "\n",
    "To assess the predictive performance of regression models, we can compute the mean sum of squared errors and the related summary metric. Furthermore, we can also use graphical approach of residual plots to diagnose problems of linear regression models\n",
    "\n",
    "We can apply regularization to our regression models to reduce the model complexity and avoid overfitting.\n",
    "\n",
    "### Non Linear relationships using Random Forests\n",
    "For the decision tree algorithm, we will subdivide the input space into smaller regions so that it's more manageable. As Decision tree algorithm does not require any transformation of the features for nonlinear data, there will not be any feature transformation in this section. Since random forests are less sensitive to outliers in the dataset we are assuming at this point that it will not require much parameter tuning. The only parameter that will require experimenting might be number of trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(mean_squared_error(predictions,targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Modeling - OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit lr using the train data and labels taken above, predict dev data.\n",
    "lr = LinearRegression()\n",
    "lr.fit(x, y)\n",
    "prediction = lr.predict(dev)\n",
    "\n",
    "## Distance of actuals vs predictors\n",
    "accuracy = abs(prediction - Y_dev)\n",
    "\n",
    "## Plots prediction vs actual sale price\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "plt.scatter(prediction, Y_dev)\n",
    "plt.title(\"Linear Regression\")\n",
    "plt.xlabel(\"Predicted Sale Price\")\n",
    "plt.ylabel(\"Actual Sale Price\")\n",
    "plt.show()\n",
    "\n",
    "## Print out the mean accuracy, rmse, and lr score.\n",
    "print(\"Mean accuracy %s \\tRMSE %s \\tRegression score %s\" \\\n",
    "    %(accuracy.mean(), rmse(prediction, Y_dev), lr.score(dev, Y_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Modeling - Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 10\n",
    "\n",
    "def rmsle_cv(model, x, test_labels):\n",
    "    # KFold is for cross-validation\n",
    "    kf = KFold(N_FOLDS, shuffle=True, random_state=0).get_n_splits(x)\n",
    "    rsle = np.sqrt(-cross_val_score(model, x, test_labels, scoring=\"neg_mean_squared_error\", cv=kf))\n",
    "    return rsle.mean()\n",
    "\n",
    "def lasso_fn(alphas):\n",
    "    rmsle_list = []\n",
    "    for i in alphas:\n",
    "        lasso_reg = Lasso(alpha=i, normalize=True)\n",
    "        rmsle_list.append(rmsle_cv(lasso_reg, x, y))\n",
    "\n",
    "    plt.plot(alphas, rmsle_list)\n",
    "    plt.xlabel(\"alpha\")\n",
    "    plt.ylabel(\"rmsle\")\n",
    "    plt.tick_params(\"L1 Lasso Model\")\n",
    "\n",
    "    print(\"The smallest rmsle by Lasso model is {:.4f}\".format(min(rmsle_list)))\n",
    "    print(\"This occurs when alpha is {}\".format(alphas[np.argmin(rmsle_list)]))\n",
    "\n",
    "    return alphas[np.argmin(rmsle_list)]\n",
    "\n",
    "ALPHAS = [0.00001, 0.0001, 0.001, 0.005, 0.01, 0.05]\n",
    "min_alpha_Lasso = lasso_fn(ALPHAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Result Analysis\n",
    "\n",
    "The first model we ran is Linear regression with L1 Regularization which is known as Lasso Model. This model outputs a sparse matrix and has built-in feature selection due to the number of coefficients that return a value of zero.[1] We picked some alpha, then evaluate the different Lasso models using k fold cross-validation. We found the optimal alpha as 0.0001 for Lasso that has an average RMSLE of 0.1189. Please note that the higher alpha is, the more coefficients become zero.[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'EN': ElasticNet(),\n",
    "    'RF': RandomForestRegressor(n_estimators=1000),\n",
    "    'GBR': GradientBoostingRegressor(n_estimators=1000, loss='huber')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'LinearRegression': { },\n",
    "    'Ridge': { 'alpha': [.001, 0.1, 1.0] },\n",
    "    'Lasso': { 'alpha': [.001, 0.1, 1.0] },\n",
    "    'EN': { 'alpha': [0.1, 1.0] },\n",
    "    'RF': {'max_depth': [4,6],\n",
    "            'min_samples_leaf': [3, 5, 9],\n",
    "            'max_features': [1.0, 0.3, 0.1]},\n",
    "    'GBR': {'learning_rate': [0.1, 0.05, 0.02],\n",
    "              'max_depth': [4, 6],\n",
    "              'min_samples_leaf': [3, 5, 9],\n",
    "              'max_features': [1.0, 0.3, 0.1]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gridcv(models,params, x, y, *, cv=3, n_jobs=1, verbose=1, scoring=None, refit=False):   \n",
    "    keys = models.keys()\n",
    "    grid_searches = {}\n",
    "    for key in keys:\n",
    "        print(\"Running GridSearchCV for %s.\" % key)\n",
    "        model = models[key]\n",
    "        param_grid = params[key]\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=cv, n_jobs=n_jobs, verbose=verbose, scoring=scoring, refit=refit)\n",
    "        grid_search.fit(x, y)\n",
    "        df0 = pd.DataFrame(grid_search.cv_results_)\n",
    "        df =pd.DataFrame(grid_search.cv_results_)[['params','mean_train_score','mean_test_score']]\n",
    "        df\n",
    "        print(df)\n",
    "        grid_searches[key] = grid_search  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_gridcv(models,params,x,y,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='partial_dependence'></a>\n",
    "## Partial Dependence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Relationship between the response and a set of features, marginalizing over all other features\n",
    "* Intuitively: expected response as a function of the features we conditioned on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGSIZE = (11,7)\n",
    "type(est.feature_importances_)\n",
    "fx_imp = pd.Series(est.feature_importances_, index=features)\n",
    "fx_imp /= fx_imp.max()  # normalize\n",
    "#fx_imp.sort()\n",
    "fx_imp.nlargest(20).plot(kind='barh', figsize=FIGSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "features = ['MasVnrArea', 'MoSold', 'MiscVal', 'FullBath', 'SaleCondition_Family','OpenPorchSF',\n",
    "            '2ndFlrSF', 'EnclosedPorch', 'WoodDeckSF','TotalBsmtSF_Log']\n",
    "fig, axs = plot_partial_dependence(est, X_train_data, features, feature_names=features, \n",
    "                                   n_cols=6, figsize=(20,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "# 7. Summary & Key Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will present our key findings in terms of key predictor variables and their parameter values. We will also summary the modeling process and learning from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "# 8. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]: <a href=\"https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\">L1 and L2 Regulazation Model</a><br>\n",
    "[2]: <a href=\"https://chrisalbon.com/machine_learning/linear_regression/effect_of_alpha_on_lasso_regression/\">Effect of Alpha on Lasso Regression</a><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
