{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project: House Prices Prediction\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques  \n",
    "\n",
    "#### Team MLAIRE: Chi Iong Ansjory, Prabhat Tripathi, Soodong Kim, Tina Agarwal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sections:\n",
    "- [1. Introduction](#Introduction)\n",
    "- [2. Import Libraries](#import_libraries)\n",
    "- [3. Data Loading](#data_loading)\n",
    "- [4. Exploratory Data Analysis](#eda)\n",
    "- [5. Feature Engineering](#feature_engineering)\n",
    "- [6. Modeling](#modeling)\n",
    "- [7. Summary & Key Results](#summary)\n",
    "- [8. References](#references)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Introduction'></a>\n",
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to predict sale price of residential homes in Ames, Iowa based on the given training and test data sets. There are 79 explanatory variables describing different aspects of residential homes with 1,460 observations in both training and test data sets. \n",
    "\n",
    "The output variable (*SalePrice*) is numeric (interval) whereas the explanatory (LHS) variables are a combination of numeric (interval) and categorical variables. \n",
    "\n",
    "Considering the size and nature of the data set, and informed by our domain understanding, we believe that *feature engineering* -- crafting data features optimized for machine learning -- is key for better modeling for this problem. Specifically, we plan to perform following:\n",
    "\n",
    "1. Load data, perform sanity check and fix any obvious data errors.  \n",
    "2. Perform exploratory data analysis (EDA) on the train data. This includes univariate and bivariate analysis of  explanatory features to understand their relationships with the output variable *SalePrice*.  \n",
    "3. Feature Engineering  \n",
    "   - Missing values and their appropriate imputation.  \n",
    "   - Transformation of features and feature scaling, if needed.  \n",
    "   - Encodings to convert categorical features into numerical features so that we can run regression models.  \n",
    "   - Split train and dev data sets.  \n",
    "4. Modeling  \n",
    "   - We will try running several regression models and compare them for their prediction results. We will choose the best model or an ensamble of candidate models for final prediction. We also have to watch out for overfitting considering there are a large number of input variables (compared to the training data size).  \n",
    "5. Summarize the key findings.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='import_libraries'></a>\n",
    "# 2. Import Libraries\n",
    "\n",
    "We import necessary libraries such as sklearn, scipy, pandas, numpy, and seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each plot will not open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# Python fundamental libraries\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from IPython.display import Image\n",
    "\n",
    "# Import relevant libraries\n",
    "\n",
    "# Top level libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# SK-learn libraries for projection/learning\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV\n",
    "\n",
    "# SK-learn libraries for data mangling\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# SK-learn libraries for evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# SK-Learn libraries for clustering experiment\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Scipy libraries\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "\n",
    "# Disable warnings for more clear output\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_loading'></a>\n",
    "# 3. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "\n",
    "We import both train and test data from csv files, make copies of both, and display the origin shapes of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shapes of train and test datasets:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1460, 81), (1459, 80))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "# Please keep this relative path access to data\n",
    "dir_name = os.getcwd()\n",
    "train_filename = os.path.join(dir_name, 'data/train.csv')\n",
    "test_filename = os.path.join(dir_name, 'data/test.csv')\n",
    "\n",
    "# Reading the data\n",
    "train = pd.read_csv(train_filename)\n",
    "test  = pd.read_csv(test_filename)\n",
    "\n",
    "# We copy and preserve the original data frames for reference\n",
    "train_orig = train.copy()\n",
    "test_orig = test.copy()\n",
    "\n",
    "print(\"Original shapes of train and test datasets:\")\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "# 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following hyperlinks direct to relevant subsections:\n",
    "- [Feature Overview](#feature_overview)\n",
    "- [Numeric Features](#numeric_features)\n",
    "- [Categorical Features](#categorical_features)\n",
    "- [Feature Summary](#features_summary)\n",
    "- [Univariate Analysis](#univariate_analysis)\n",
    "    - [SalePrice](#saleprice)\n",
    "    - [GrLivArea](#grlivarea)\n",
    "    - [LotArea](#lotarea)\n",
    "    - [PoolArea](#poolarea)\n",
    "    - [OverallQual](#overallqual)\n",
    "    - [YearBuilt](#yearbuilt)\n",
    "    - [Street](#street)\n",
    "    - [MiscFeature](#miscfeature)\n",
    "    - [MiscVal](#miscval)\n",
    "    - [YrSold](#yrsold)\n",
    "    - [MoSold](#mosold)\n",
    "- [Bivariate Analysis](#bivariate_analysis)\n",
    "    - [SalePrice - GrLivArea Relationship](#saleprice_grlivarea)\n",
    "    - [SalePrice - OverallQual Relationship](#saleprice_overallqual)\n",
    "    - [SalePrice - YearBuilt Relationship](#saleprice_yearbuilt)\n",
    "    - [SalePrice - GarageCars Relationship](#saleprice_garagecars)\n",
    "    - [SalePrice - BsmtQual Relationship](#saleprice_bsmtqual)\n",
    "    - [SalePrice - BsmtCond Relationship](#saleprice_bsmtcond)\n",
    "    - [SalePrice - BsmtExposure Relationship](#saleprice_bsmtexposure)\n",
    "    - [SalePrice - KitchenQual Relationship](#saleprice_kitchenqual)\n",
    "    - [SalePrice - ExterQual Relationship](#saleprice_exterqual)\n",
    "    - [SalePrice - Picked Variables Relationship](#saleprice_pickedvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities    ...     PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "0         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "1         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "2         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "3         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "4         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "\n",
       "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008        WD         Normal     208500  \n",
       "1      5   2007        WD         Normal     181500  \n",
       "2      9   2008        WD         Normal     223500  \n",
       "3      2   2006        WD        Abnorml     140000  \n",
       "4     12   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A glimpse of the data set\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feature_overview'></a>\n",
    "## Feature Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin with discriminating between numeric and categorical features\n",
    "# (including SalePrice output variable)\n",
    "numeric_columns = [f for f in train.columns if train.dtypes[f] != 'object']\n",
    "category_columns = [f for f in train.columns if train.dtypes[f] == 'object']\n",
    "print('Number of numeric features: {}'.format(len(numeric_columns)))\n",
    "print('Number of categorical features: {}'.format(len(category_columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='numeric_features'></a>\n",
    "## Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-identifying numeric features\n",
    "numeric_columns = [f for f in train.columns if train.dtypes[f] != 'object']\n",
    "print('* Number of numeric features: {}'.format(len(numeric_columns)))\n",
    "\n",
    "# Print numeric features and some properties\n",
    "print(\"-------------------------------- Numeric Features --------------------------------\")\n",
    "print(numeric_columns)\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "\n",
    "# We also look at numeric features that have small number of unique \"discrete\" values\n",
    "discrete_columns = []\n",
    "print(\"\\n* Discrete features (Numeric features with small number of unique values)\")\n",
    "for col in numeric_columns:\n",
    "    if (train[col].nunique() <= 10):\n",
    "        discrete_columns.append(col)\n",
    "        print ('{}: {}'.format(col, train[col].nunique()))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='categorical_features'></a>\n",
    "## Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_columns = [f for f in train.columns if train.dtypes[f] == 'object']\n",
    "print('* Number of categorical features: {}'.format(len(category_columns)))\n",
    "# print Categorical feature names\n",
    "print(\"------------------------------ Categorical Features ------------------------------\")\n",
    "print(category_columns)\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "\n",
    "# For ordinal variables, from data description and exploration, we observed several categorical\n",
    "# variables whose values have order. This may be important in modeling where we treat these\n",
    "# variables differently.\n",
    "ordinals_columns = ['KitchenQual', 'Fence', 'Functional', 'LotShape','LandSlope','ExterQual',\n",
    "                    'ExterCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n",
    "                    'BsmtFinType2','HeatingQC','Electrical','FireplaceQu','GarageFinish',\n",
    "                    'GarageQual','GarageCond','PavedDrive','PoolQC']\n",
    "print(\"\\n* Candidate Ordinal features (Categorical features with small number of unique values)\")\n",
    "for col in ordinals_columns:\n",
    "    if (train[col].nunique() <= 10):\n",
    "        print ('{}: {}, {}'.format(col, train[col].nunique(), train[col].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will understand the relationship of candidate ordinal features and *SalePrice* to determine if they are truly ordinal features. We will do this check by understanding the relationship between a feature and *SalePrice*. If *SalePrice* increases with the order of values in the a feature, we can model the feature as ordinal feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='features_summary'></a>\n",
    "## Feature Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above analysis, we categorized features into 4 categories. Numeric, Numeric (Discrete), Categorical, Catagorical (Ordinal).  \n",
    "\n",
    "The following table summarizes all the features by including important properties for each of them.  \n",
    "\n",
    "Descriptions of variables are based on https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data. Variable types are either categorical, or discrete/non-discrete numeric. There are 4 different \"Segments\": Sale (sale transaction of house), Location (where the house located), Building (physical characteristics), Space (space properties of house). For numerical variables, mean, min, max, and number of NaNs are derived from descriptive statistics, and also determine if it is discrete from the histogram. For categorical variables, number of NaNs and distinct values are determined by histogram. There are 3 levels of \"Expectations\": High, Medium, and Low of how the variables are related to the sale price. \n",
    "\n",
    "|  | Variable | Description | Type | Segment | Mean | Min | Max | NaN | Distinct Value | Expectated Relevance | Comments |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 1 | SalePrice | Property's sale price in dollars | Numeric | Building  | 180921 | 34900 | 755000 | 0 | | | Output variable |\n",
    "| 2 | MSSubClass | Building class | Categorical| Building  | | | | 0 | 15 | Low |  |\n",
    "| 3 | MSZoning | General zoning classification | Categorical | Building | | | | 0 | 5  | Low |  |\n",
    "| 4 | LotFrontage | Linear feet of street connected to property | Numeric | Space |70 | 21 | 313 | 259 | | Medium |  |\n",
    "| 5 | LotArea | Lot size in square feet | Numeric | Space | 10516 | 1300 | 215245 | 0 | | High |  |\n",
    "| 6 | Street | Type of road access | Categorical | Location | | | | 0 | 2 | Low |  |\n",
    "| 7 | Alley | Type of alley access | Categorical | Location | | | | | 3 | Low | Most values NAs |\n",
    "| 8 | LotShape | General shape of property |Categorical (Ordinal) | Location |  | | | 0 | 4 | Low |  |\n",
    "| 9 | LandContour | Flatness of the property | Categorical | Location |  | | | 0 | 9 | Low |  |\n",
    "| 10 | Utilities | Type of utilities available |Categorical |  Location |  | | | 0 | 1 | Low | Almost all values same (AllPub) |\n",
    "| 11 | LotConfig | Lot configuration |Categorical | Location |  | | | 0 | 5 | Medium |  |\n",
    "| 12 | LandSlope | Slope of property | Categorical (Ordinal)  | Location | | | | 0 | 3 | Low |  |\n",
    "| 13 | Neighborhood | Physical locations within Ames city limits | Categorical | Location  |  | | | 0 | 25 | Medium |  |\n",
    "| 14 | Condition1 | Proximity to main road or railroad |Categorical | Location | | | | 0 | 9 | Medium |  |\n",
    "| 15 | Condition2 | Proximity to main road or railroad (if a second is present) | Categorical | Location | | | | 0 | 2| Low |  |\n",
    "| 16 | BldgType | Type of dwelling | Categorical | Building | | | | 0 | 5 | Low |  |\n",
    "| 17 | HouseStyle | Style of dwelling |Categorical | Building |  | | | 0 | 8 | Low |  |\n",
    "| 18 | OverallQual | Overall material and finish quality | Numeric (Discrete) | Building |6 | 1 | 10 | 0 | | High |  |\n",
    "| 19 | OverallCond | Overall condition rating | Numeric (Discrete)  | Building | 5 | 1 | 9 | 0 | | Medium |  |\n",
    "| 20 | YearBuilt | Original construction date | Categorical |Building  | | | | 0 | 112 | High |  |\n",
    "| 21 | YearRemodAdd | Remodel date | Categorical | Building  | | | | 0 | 61 | High |  |\n",
    "| 22 | RoofStyle | Type of roof | Categorical | Building | | | | 0 | 5 | Medium |  |\n",
    "| 23 | RoofMatl | Roof material | Categorical | Building | | | | 0 | 4 | Low |  |\n",
    "| 24 | Exterior1st | Exterior covering on house | Categorical | Building | | | | 0 | 12 | Medium |  |\n",
    "| 25 | Exterior2nd | Exterior covering on house (if more than one material) | Categorical | Building | | | | 0 | 16| Low |  |\n",
    "| 26 | MasVnrType | Masonry veneer type | Categorical | Building  |  | | | 0 | 4 | Low |  |\n",
    "| 27 | MasVnrArea | Masonry veneer area in square feet | Numeric| Space  | 103 | 0 | 1600 | 8 | 0 | Low |  |\n",
    "| 28 | ExterQual | Exterior material quality | Categorical (Ordinal) | Building | | | | 0 | 4 | Low |  |\n",
    "| 29 | ExterCond | Present condition of the material on the exterior | Categorical (Ordinal) | Building  | | | | 0 | 3 | Medium  |  |\n",
    "| 30 | Foundation | Type of foundation | Categorical | Building | | | | 0 | 6 | Low | |\n",
    "| 31 | BsmtQual | Evaluates the height of the basement | Categorical (Ordinal) | Building | | | | 0 | 4 | Low |  |\n",
    "| 32 | BsmtCond | General condition of the basement | Categorical (Ordinal) | Building | | | | 0 | 3 | Medium |  |\n",
    "| 33 | BsmtExposure | Walkout or garden level basement walls | Categorical (Ordinal)  | Building | | | | 0 | 4 | Low |  |\n",
    "| 34 | BsmtFinType1 | Quality of basement finished area | Categorical (Ordinal)  | Building  | | | | 0 | 6 | Low |  |\n",
    "| 35 | BsmtFinSF1 | Type 1 finished square feet | Numeric | Space |443 | 0 | 5644 | 0 | | Medium |  |\n",
    "| 36 | BsmtFinType2 | Quality of second finished area (if present) | Categorical (Ordinal)  | Building | | | | 0 | 6 | Low |  |\n",
    "| 37 | BsmtFinSF2 | Type 2 finished square feet | Numeric | Space | 46 | 0 | 1474 | 0 | | Low |  |\n",
    "| 38 | BsmtUnfSF | Unfinished square feet of basement area | Numeric | Space | 567 | 0 | 2336 | 0 | | Medium |  |\n",
    "| 39 | TotalBsmtSF | Total square feet of basement area | Numeric | Space | 1057 | 0 | 6110 | 0 | | High |  |\n",
    "| 40 | Heating | Type of heating | Categorical | Building  | | | | 0 | 3 | Low |  |\n",
    "| 41 | HeatingQC | Heating quality and condition | Categorical (Ordinal)  | Building | | | | 0 | 5 | Low |  |\n",
    "| 42 | CentralAir | Central air conditioning | Categorical | Building  | | | | 0 | 2 | Medium |  |\n",
    "| 43 | Electrical | Electrical system | Categorical (Ordinal)  | Building | | | | 1 | 5 | Low |  |\n",
    "| 44 | 1stFlrSF | First Floor square feet | Numeric | Space | 1162 | 334 | 4692 | 0 | | High |  |\n",
    "| 45 | 2ndFlrSF | Second floor square feet | Numeric | Space | 346 | 0 | 2065 | 0 | | Medium |  |\n",
    "| 46 | LowQualFinSF | Low quality finished square feet (all floors) | Numeric | Space | 5 | 0 | 572 | 0 | | Low |  |\n",
    "| 47 | GrLivArea | Above grade (ground) living area square feet | Numeric | Space | 1515 | 334 | 5642 | 0 | | High |  |\n",
    "| 48 | BsmtFullBath | Basement full bathrooms | Numeric (Discrete) | Building | 0 | 0 | 3 | 0 | | Low |  |\n",
    "| 49 | BsmtHalfBath | Basement half bathrooms | Numeric (Discrete) | Building | 0 | 0 | 2 | 0 | | Low |  |\n",
    "| 50 | FullBath | Full bathrooms above grade | Numeric (Discrete) | Building | 1 | 0 | 3 | 0 | | High |  |\n",
    "| 51 | HalfBath | Half baths above grade | Numeric (Discrete) | Building |  0 | 0 | 2 | 0 | | Low |  |\n",
    "| 52 | BedroomAbvGr | Number of bedrooms above basement level | Numeric (Discrete) | Building | 2 | 0 | 8 | 0 | | Medium |  |\n",
    "| 53 | KitchenAbvGr | Number of kitchens | Numeric (Discrete) | Building | 1 | 0 | 3 | 0 | | Low |  |\n",
    "| 54 | KitchenQual | Kitchen quality | Categorical (Ordinal) | Building | | | | 0 | 4 | Medium |  |\n",
    "| 55 | TotRmsAbvGrd | Total rooms above grade (does not include bathrooms) | Numeric | Building  | 6 | 2 | 14 | 0 | | High |  |\n",
    "| 56 | Functional | Home functionality rating | Categorical (Ordinal) | Building | | | | 0 | 7 | Low |  |\n",
    "| 57 | Fireplaces | Number of fireplaces | Numeric (Discrete) | Building | 0 | 0 | 3 | 0 | | Medium |  |\n",
    "| 58 | FireplaceQu | Fireplace quality | Categorical (Ordinal)  | Building | | | | 690 | 5 | Low |  |\n",
    "| 59 | GarageType | Garage location | Categorical | Building | | | | 81 | 6 | Low |  |\n",
    "| 60 | GarageYrBlt | Year garage was built | Categorical | Building | | | | 81 | 97 | Low |  |\n",
    "| 61 | GarageFinish | Interior finish of the garage | Categorical (Ordinal)  | Building | | | | 81 | 3 | Low |  |\n",
    "| 62 | GarageCars | Size of garage in car capacity | Numeric (Discrete) | Building | 1 | 0 | 4 | 0 | | High |  |\n",
    "| 63 | GarageArea | Size of garage in square feet | Numeric | Space | 472 | 0 | 1418 | 0 | | High |  |\n",
    "| 64 | GarageQual | Garage quality | Categorical (Ordinal)  | Building | | | | 81 | 5 | Low |  |\n",
    "| 65 | GarageCond | Garage condition | Categorical (Ordinal)  | Building | | | | 81 | 5 | Low |  |\n",
    "| 66 | PavedDrive | Paved driveway | Categorical (Ordinal)  | Building | | | | 0 | 3 | Medium |  |\n",
    "| 67 | WoodDeckSF | Wood deck area in square feet | Numeric | Space | 94 | 0 | 857 | 0 | | Low |  |\n",
    "| 68 | OpenPorchSF | Open porch area in square feet | Numeric | Space | 46 | 0 | 547 | 0 | | Low |  |\n",
    "| 69 | EnclosedPorch | Enclosed porch area in square feet | Numeric | Space | 21 | 0 | 552 | 0 | | Low |  |\n",
    "| 70 | 3SsnPorch | Three season porch area in square feet | Numeric | Space | 3 | 0 | 508 | 0 | | Low |  |\n",
    "| 71 | ScreenPorch | Screen porch area in square feet | Numeric | Space | 15 | 0 | 480 | 0 | | Low |  |\n",
    "| 72 | PoolArea | Pool area in square feet | Numeric (Discrete) | Space | 2 | 0 | 738 | 0 | | Medium |  |\n",
    "| 73 | PoolQC | Pool quality | Categorical (Ordinal)  | Building | | | | 1453 | 3 | Low |  |\n",
    "| 74 | Fence | Fence quality | Categorical (Ordinal) | Building | | | | 1179 | 4 | Low |  |\n",
    "| 75 | MiscFeature | Miscellaneous feature not covered in other categories | Categorical | Building | | | | 1406 | 4 | Low |  |\n",
    "| 76 | MiscVal | $Value of miscellaneous feature | Numeric | Building | 43 | 0 | 15500 | 0 | | Low |  |\n",
    "| 77 | MoSold | Month Sold | Numeric | Sale | | | | 0 | 12 | Medium | Need to model as categorical |\n",
    "| 78 | YrSold | Year Sold | Numeric | Sale | | | | 0 | 5 | Medium | Need to model as categorical |\n",
    "| 79 | SaleType | Type of sale | Categorical | Sale  | | | | 0 | 9 | Medium |  |\n",
    "| 80 | SaleCondition | Condition of sale | Categorical | Sale | | | | 0 | 6 | Medium | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='univariate_analysis'></a>\n",
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will look at several key columns (variables) individually to understand them better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='saleprice'></a>\n",
    "### SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, it is essential to analyze the characteristics and distribution of the output variable to see if any transformation is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "round(train['SalePrice'].describe(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram and normal probability plot\n",
    "plt.subplots(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(train['SalePrice'], fit=norm).set_title(\"Histogram (before)\");\n",
    "plt.subplot(1, 2, 2)   \n",
    "stats.probplot(train['SalePrice'], plot=plt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness and Kurtosis\n",
    "print(\"Skewness: \", round(train['SalePrice'].skew(), 2))\n",
    "print(\"Kurtosis: \", round(train['SalePrice'].kurt(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the target variable *SalePrice* has a right-skewed distribution. Log transformation appears to reduce the skewness. A normally distributed target variable helps in better modeling the relationship between target and independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying log transformation\n",
    "plt.subplots(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(np.log(train['SalePrice']), fit=norm).set_title(\"Histogram (after)\");\n",
    "plt.subplot(1, 2, 2)   \n",
    "stats.probplot(np.log(train['SalePrice']), plot=plt);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above log transformation of the target variable helps reduce skewed distribution and the new distribution looks closer to normal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we start to analyze a few explanatory variables marked with high expected relevance from feature summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='grlivarea'></a>\n",
    "### GrLivArea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*GrLivArea*, above grade living area size, is certainly the most relevant numeric variable. There is a huge gap between 75 percentile and maximum value. This indicates that there could be some outliers that we may consider dropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "round(train['GrLivArea'].describe(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at the histogram to understand the distribution. It has a long tail with slight right skew. Then, based on the probabilty plot, there are a few outliers seen when *GrLivArea* is greater than 4000, which are candidates for dropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram and normal probability plot\n",
    "plt.subplots(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(train['GrLivArea'], fit=norm).set_title(\"Histogram\");\n",
    "plt.subplot(1, 2, 2)   \n",
    "stats.probplot(train['GrLivArea'], plot=plt);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like log transformation is needed to reduce the skewness and improve normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lotarea'></a>\n",
    "### LotArea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*LotArea*, lot size, is the next most relevant numeric variable. There is a huge gap between 75 percentile and maximum value. This indicates that there could be some outliers that we may consider dropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "round(train['LotArea'].describe(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at the histogram to understand the distribution. It has a long tail with slight right skew. Then, based on the probabilty plot, there are a few outliers seen when *LotArea* is greater than 75,000, which are candidates for dropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram and normal probability plot\n",
    "plt.subplots(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(train['LotArea'], fit=norm).set_title(\"Histogram\");\n",
    "plt.subplot(1, 2, 2)   \n",
    "stats.probplot(train['LotArea'], plot=plt);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram demonstrates normal distribution but has a long tail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='poolarea'></a>\n",
    "### PoolArea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*PoolArea* is an interesting numeric variable since majority of the properties don't have a pool and the area should be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "round(train['PoolArea'].describe(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram and normal probability plot\n",
    "plt.subplots(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(train['PoolArea'], fit=norm).set_title(\"Histogram\");\n",
    "plt.subplot(1, 2, 2)   \n",
    "stats.probplot(train['PoolArea'], plot=plt);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the probability plot, only a handful of properties have swimming pools but majority are without one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='overallqual'></a>\n",
    "### OverallQual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*OverallQual* is an example of most relevant discrete numeric variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "round(train['OverallQual'].describe(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "sns.distplot(train['OverallQual']).set_title(\"Histogram\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the discrete nature of *OverallQual* feature shows multi-model histogram with a close to normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='yearbuilt'></a>\n",
    "### YearBuilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YearBuilt* is one of the most relevant categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "sns.distplot(train['YearBuilt']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram sorted in descending order\n",
    "train['YearBuilt'].value_counts().plot(kind='bar', figsize=(20, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram with years in chronological order doesn't display any observable relationship. However, after sorted the number of houses in descending order, the histogram appears to follow a power curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will look at a few least relevant variables and see why they definitely need to be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='street and miscfeature'></a>\n",
    "### Street and MiscFeature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Street* doesn't give any meaningful information since almost all values are \"Pave\".  \n",
    "- With 99.6% of properties have identical *Pave* type, *Street* is a candidate to be dropped.  \n",
    "- Less than 4% of properties come with this *MiscFeature*, and it is a candidate to be dropped.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "train['Street'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted in descending order\n",
    "plt.subplots(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)  \n",
    "train['Street'].value_counts().plot(kind='bar');\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Sorted in descending order\n",
    "train['MiscFeature'].value_counts().plot(kind='bar'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='miscval'></a>\n",
    "### MiscVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*MiscVal* doesn't give any meaningful information since most of properties come with this value and almost all values are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted in descending order\n",
    "train['MiscVal'].value_counts().plot(kind='bar', figsize=(5,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than 95% of properties have *MiscVal* with $0, and it is a candidate to be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='yrsold and MoSold'></a>\n",
    "### YrSold and MoSold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YrSold* appears to be discrete numeric variable and it is a candiate to be converted to categorical.\n",
    "*MoSold* appears to be discrete numeric variable and it is a candiate to be converted to categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15, 5))\n",
    "plt.subplot(1, 2,1 ) \n",
    "# Histogram\n",
    "sns.distplot(train['YrSold']);\n",
    "plt.subplot(1, 2, 2) \n",
    " # Histogram\n",
    "sns.distplot(train['MoSold']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bivariate_analysis'></a>\n",
    "## Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we look at the relationship between target variable (*SalePrice*) and some other relevant features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix showing evidence of multicollearity\n",
    "# Remove Id column which is totally irrelevant\n",
    "train_ = train.drop(['Id'], axis=1, errors='raise')\n",
    "corrmat = train_.corr()\n",
    "\n",
    "# Saleprice correlation matrix\n",
    "k = 10 # number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "cm = np.corrcoef(train_[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "plt.figure(figsize=(8, 8))\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f',\n",
    "                 annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()\n",
    "\n",
    "# Let us print top correlated features (both positively and negatively) with SalePrice\n",
    "corr_matrix = train_.corr()\n",
    "print (corr_matrix['SalePrice'].sort_values(ascending=False)[:10], '\\n') # top 10 values\n",
    "print ('----------------------')\n",
    "print (corr_matrix['SalePrice'].sort_values(ascending=False)[-10:]) # bottom 10 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the correlation matrix (lighter color -> better correlation), some of the highly correlated features appear to be *OverallQual*, *YearBuilt*, *YearRemodAdd*, *TotalBsmtSF*, *1stFlrSF*, *GrLivArea*, *FullBath*, *TotRmsAbvGrd*, *GarageCars*, and *GarageArea*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, some variables seem to be strongly correlated with the target variable. Here we see that the *OverallQual* feature is 79% correlated with the target variable. *OverallQual* feature Rates the overall material and finish of the house. This seems to be the parameter that affects the sale price positively. In addition, *GrLivArea* is 70% correlated with the target variable. *GrLivArea* refers to the living area (in sq ft.) above ground."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, based on the correlation matrix, multicollinearity is seen on a few pair of variables. For example, *GarageArea* and *GarageCars*; *TotRmsAbvGrd* and *GrLivArea*; *TotalBsmtSF* and *1stFlrSF*. *GarageCars* and *TotRmsAbvGrd* are candiates to be dropped since categorical variables will be weeker against the numeric variables in the modeling. *TotalBsmtSF* is candidate to be dropped since there are more explicit variables to describe basements such as *BsmtFinSF1*, *BsmtFinSFs*, and *BsmtUnfSF*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='saleprice_grlivarea'></a>\n",
    "### SalePrice - GrLivArea Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since *GrLivArea* is a numeric variable, we look at the scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot of GrLivArea/SalePrice\n",
    "data = pd.concat([train['SalePrice'], train_['GrLivArea']], axis=1)\n",
    "data.plot.scatter(x='GrLivArea', y='SalePrice', ylim=(0, 800000));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*GrLivArea* and *SalePrice* seem to have linear relationship of larger the area, higher the price. However, there are a few observable outliers, though, on the higher extremes of areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='saleprice_overallqual'></a>\n",
    "### SalePrice - OverallQual Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since *OverallQual* is a discrete variable, we look at the boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of OverallQual/SalePrice\n",
    "var = 'OverallQual'\n",
    "data = pd.concat([train['SalePrice'], train_[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n",
    "fig.axis(ymin=0, ymax=800000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot shows strong relationship between overall quality of the house and its sale price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='saleprice_yearbuilt'></a>\n",
    "### SalePrice - YearBuilt Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, boxplot also shows the general trend of higher price for newer house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of YearBuilt/SalePrice\n",
    "var = 'YearBuilt'\n",
    "data = pd.concat([train['SalePrice'], train_[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(20, 8))\n",
    "fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n",
    "fig.axis(ymin=0, ymax=800000)\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='saleprice_garagecars'></a>\n",
    "### SalePrice - GarageCars Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot shows that there is linear relationship between size of garage and price when garage size is between 0 and 3. House with 4-car garage, though, doesn't appears to positively influence sale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of GarageCars/SalePrice\n",
    "data = pd.concat([train['SalePrice'], train_['GarageCars']], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 4))\n",
    "fig = sns.boxplot(x='GarageCars', y=\"SalePrice\", data=data)\n",
    "fig.axis(ymin=0, ymax=800000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='saleprice_bsmtqual'></a>\n",
    "### SalePrice - BsmtQual Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since *BsmtQual* is a discrete variable, we look at the boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of BsmtQual/SalePrice\n",
    "data = pd.concat([train['SalePrice'], train_['BsmtQual']], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 4))\n",
    "fig = sns.boxplot(x='BsmtQual', y=\"SalePrice\", data=data, order=[\"Ex\", \"Gd\", \"TA\", \"Fa\"])\n",
    "fig.axis(ymin=0, ymax=800000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows a linear relationship between sale price and different tier of basement quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='saleprice_bsmtcond'></a>\n",
    "### SalePrice - BsmtCond Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since *BsmtCond* is a discrete variable, we look at the boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of BsmtCond/SalePrice\n",
    "data = pd.concat([train['SalePrice'], train_['BsmtCond']], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 4))\n",
    "fig = sns.boxplot(x='BsmtCond', y=\"SalePrice\", data=data,\n",
    "                  order=[\"Ex\", \"Gd\", \"TA\", \"Fa\"])\n",
    "fig.axis(ymin=0, ymax=800000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows a linear relationship between sale price and different tier of basement condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='saleprice_bsmtexposure'></a>\n",
    "### SalePrice - BsmtExposure Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since *BsmtExposure* is a discrete variable, we look at the boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of BsmtExposure/SalePrice\n",
    "data = pd.concat([train['SalePrice'], train_['BsmtExposure']], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 4))\n",
    "fig = sns.boxplot(x='BsmtExposure', y=\"SalePrice\", data=data,\n",
    "                  order=[\"Gd\", \"Av\", \"Mn\", \"No\"])\n",
    "fig.axis(ymin=0, ymax=800000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows a linear relationship between sale price and different tier of basement exposure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='saleprice_kitchenqual'></a>\n",
    "### SalePrice - KitchenQual Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since *KitchenQual* is a discrete variable, we look at the boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of KitchenQual/SalePrice\n",
    "data = pd.concat([train['SalePrice'], train_['KitchenQual']], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 4))\n",
    "fig = sns.boxplot(x='KitchenQual', y=\"SalePrice\", data=data,\n",
    "                  order=[\"Ex\", \"Gd\", \"TA\", \"Fa\"])\n",
    "fig.axis(ymin=0, ymax=800000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows a linear relationship between sale price and different tier of kitchen quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='saleprice_exterqual'></a>\n",
    "### SalePrice - ExterQual Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since *ExterQual* is a discrete variable, we look at the boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of ExterQual/SalePrice\n",
    "data = pd.concat([train['SalePrice'], train_['ExterQual']], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 4))\n",
    "fig = sns.boxplot(x='ExterQual', y=\"SalePrice\", data=data,\n",
    "                  order=[\"Ex\", \"Gd\", \"TA\", \"Fa\"])\n",
    "fig.axis(ymin=0, ymax=800000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows a linear relationship between sale price and different tier of exterior quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='saleprice_pickedvars'></a>\n",
    "### SalePrice - Picked Variables Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatterplot of *SalePrice* vs the 10 picked variables, look like all demonstrate linear relationship with sale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot\n",
    "sns.set()\n",
    "cols = ['SalePrice','OverallQual','YearBuilt','YearRemodAdd','TotalBsmtSF',\n",
    "        '1stFlrSF','GrLivArea','FullBath', 'TotRmsAbvGrd', 'GarageCars',\n",
    "        'GarageArea']\n",
    "sns.pairplot(train_[cols], size=2.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feature_engineering'></a>\n",
    "# 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following hyperlinks direct to relevant subsections:\n",
    "- [Drop Outlier Rows](#drop_outlier_row)\n",
    "- [Experiment Data Setup for Clustering analyssi](#experiment_data_setup) \n",
    "- [Intepretation for Experiment on Clustering](#interpretation_for_experiment)\n",
    "- [Combine Train and Test Data](#combining_data)\n",
    "- [Remove Columns that do not help](#remove_bad_columns)\n",
    "- [Missing Data Analysis](#missing_data_analysis)\n",
    "- [Missing Data Handling](#missing_data_handling)\n",
    "- [Missing Values Handling](#impute_NaN_values)\n",
    "- [Adding New Columns (Features)](#add_new_cols)\n",
    "- [Data Preparation](#data_preparation)\n",
    "    - [Log Transformations](#log_transformations)\n",
    "    - [Manually Encoding Ordinal Features](#ordinal_encoding)\n",
    "    - [Data Binning](#data_binning)\n",
    "    - [Cleanup For Binning Applied Columns](#binning_cleanup)\n",
    "    - [Categorical Features: Conversion to Dummy/Indicator Variables](#categorical_encoding)\n",
    "- [Split Back into Train and Test](#split_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='drop_outlier_row'></a>\n",
    "## Drop Outlier Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From univariate analysis we saw that a couple of items with *GrLivArea* > 4,000 are outliers and may not help generalizing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(train[(train['GrLivArea'] > 4000)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='experiment_data_setup'></a>\n",
    "### Experiment Data Setup for Clustering analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before feature engineering, we will can check whether some columns can be used for experiments. First of all, we take a careful look on Neighbhorhood. <br>\n",
    "Please note that clustering experimental data setup will not have an impact on the following model analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping neighborhood variable based on this plot\n",
    "train['SalePrice'].groupby(train['Neighborhood']).median().sort_values().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and keep the SalePrice (target) column\n",
    "Y_train_neigh = train['SalePrice']\n",
    "X_train_neigh = train['Neighborhood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new data\n",
    "train_new = train[train['SalePrice'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The graph above gives us a good hint on how to combine levels of the neighborhood variable into fewer levels. \n",
    "#We can combine bars of equal height in one category. \n",
    "#To do this, create a dictionary and map it with variable values. \n",
    "\n",
    "neighborhood_map = {\"MeadowV\" : 0, \"IDOTRR\" : 1, \"BrDale\" : 1, \"OldTown\" : 1, \"Edwards\" : 1, \"BrkSide\" : 1, \"Sawyer\" : 1, \"Blueste\" : 1, \"SWISU\" : 2, \"NAmes\" : 2, \"NPkVill\" : 2, \"Mitchel\" : 2, \"SawyerW\" : 2, \"Gilbert\" : 2, \"NWAmes\" : 2, \"Blmngtn\" : 2, \"CollgCr\" : 2, \"ClearCr\" : 3, \"Crawfor\" : 3, \"Veenker\" : 3, \"Somerst\" : 3, \"Timber\" : 3, \"StoneBr\" : 4, \"NoRidge\" : 4, \"NridgHt\" : 4}\n",
    "\n",
    "train_new['NeighborhoodBin'] = train_new['Neighborhood'].map(neighborhood_map)\n",
    "train_new.loc[train_new.Neighborhood == 'NridgHt', \"Neighborhood_Good\"] = 1\n",
    "train_new.loc[train_new.Neighborhood == 'Crawfor', \"Neighborhood_Good\"] = 1\n",
    "train_new.loc[train_new.Neighborhood == 'StoneBr', \"Neighborhood_Good\"] = 1\n",
    "train_new.loc[train_new.Neighborhood == 'Somerst', \"Neighborhood_Good\"] = 1\n",
    "train_new.loc[train_new.Neighborhood == 'NoRidge', \"Neighborhood_Good\"] = 1\n",
    "train_new[\"Neighborhood_Good\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols =['NeighborhoodBin','Neighborhood_Good']\n",
    "train_new = train_new[cols]\n",
    "test_new = train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_2= PCA()\n",
    "x_pc = pc_2.fit_transform(train_new)\n",
    "plt.scatter(x_pc[:,0],x_pc[:,1], label='True Position')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans (n_clusters=4, init='k-means++')\n",
    "clstrs2 = km.fit(train_new)\n",
    "y_kmeans = km.predict(train_new)\n",
    "print(km.labels_)\n",
    "print(clstrs2.cluster_centers_.shape)\n",
    "print(clstrs2.cluster_centers_)\n",
    "plt.scatter(x_pc[:, 0], x_pc[:, 1], c=km.labels_,cmap='rainbow')\n",
    "\n",
    "centers = km.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], color='black', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='interpretation_for_experiment'></a>\n",
    "### Intepretation for Experiment on Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before various models analysis, we used clustering to see whether some grouped coulumns based on the specific characteristic may lead different projection. For neighborhood, we assumed that higher sales price may be based on better school districts having Ames Hish School which is ranked 3 in Iowa (https://www.usnews.com/education/best-high-schools/iowa/districts/ames-comm-school-district/ames-high-school-7498).<br>\n",
    "However, neighborhood which is closed to the above high schools are Somerset and Bloomington Height. However, they are not top ones in the analysis. Hence, clustering through school zone for neighborhood might not have big impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='combining_data'></a>\n",
    "## Combine Train and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will combine train and test data frames so that all feature engineering changes are applied to both together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 81)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract and keep the SalePrice (target) column\n",
    "Y_train_all = train['SalePrice']\n",
    "# Drop SalePrice so that is has same cardinality as test dataframe\n",
    "train.drop('SalePrice', axis=1, inplace=True)\n",
    "\n",
    "# Preseve test dataset IDs. We will need them for final prediction/submission\n",
    "test_id = test.iloc[:,0]\n",
    "\n",
    "# Combine test and train\n",
    "combined_df = pd.concat([train, test]).reset_index()\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='remove_bad_columns'></a>\n",
    "## Remove Columns that do not help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the univariate analysis, we also found that these columns does not help explain the variations in *SalePrice*\n",
    "removing these as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID is not useful\n",
    "combined_df.drop('Id', axis=1, inplace=True)\n",
    "\n",
    "# Almost all rows have same value for Utilities, making this variable not useful\n",
    "combined_df.drop('Utilities', axis=1, inplace=True)\n",
    "combined_df.drop('Street', axis=1, inplace=True)\n",
    "combined_df.drop('MiscFeature', axis=1, inplace=True)\n",
    "combined_df.drop('MiscVal', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the bivariate analysis, we found some dditional variables could be dropped to avoid multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.drop(['GarageCars','TotRmsAbvGrd','TotalBsmtSF'],axis=1,\n",
    "                 inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='missing_data_analysis'></a>\n",
    "## Missing Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will analyze missing values in the data set and approach to tackle these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Number of Missing Data</th>\n",
       "      <th>Missing Data Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PoolQC</th>\n",
       "      <td>2909</td>\n",
       "      <td>99.657417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alley</th>\n",
       "      <td>2721</td>\n",
       "      <td>93.216855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fence</th>\n",
       "      <td>2348</td>\n",
       "      <td>80.438506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FireplaceQu</th>\n",
       "      <td>1420</td>\n",
       "      <td>48.646797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LotFrontage</th>\n",
       "      <td>486</td>\n",
       "      <td>16.649538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageFinish</th>\n",
       "      <td>159</td>\n",
       "      <td>5.447071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <td>159</td>\n",
       "      <td>5.447071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageCond</th>\n",
       "      <td>159</td>\n",
       "      <td>5.447071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageQual</th>\n",
       "      <td>159</td>\n",
       "      <td>5.447071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageType</th>\n",
       "      <td>157</td>\n",
       "      <td>5.378554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtCond</th>\n",
       "      <td>82</td>\n",
       "      <td>2.809181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtExposure</th>\n",
       "      <td>82</td>\n",
       "      <td>2.809181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtQual</th>\n",
       "      <td>81</td>\n",
       "      <td>2.774923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinType2</th>\n",
       "      <td>80</td>\n",
       "      <td>2.740665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <td>79</td>\n",
       "      <td>2.706406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MasVnrType</th>\n",
       "      <td>24</td>\n",
       "      <td>0.822199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MasVnrArea</th>\n",
       "      <td>23</td>\n",
       "      <td>0.787941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSZoning</th>\n",
       "      <td>4</td>\n",
       "      <td>0.137033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Functional</th>\n",
       "      <td>2</td>\n",
       "      <td>0.068517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <td>2</td>\n",
       "      <td>0.068517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Total Number of Missing Data  Missing Data Percentage\n",
       "PoolQC                                2909                99.657417\n",
       "Alley                                 2721                93.216855\n",
       "Fence                                 2348                80.438506\n",
       "FireplaceQu                           1420                48.646797\n",
       "LotFrontage                            486                16.649538\n",
       "GarageFinish                           159                 5.447071\n",
       "GarageYrBlt                            159                 5.447071\n",
       "GarageCond                             159                 5.447071\n",
       "GarageQual                             159                 5.447071\n",
       "GarageType                             157                 5.378554\n",
       "BsmtCond                                82                 2.809181\n",
       "BsmtExposure                            82                 2.809181\n",
       "BsmtQual                                81                 2.774923\n",
       "BsmtFinType2                            80                 2.740665\n",
       "BsmtFinType1                            79                 2.706406\n",
       "MasVnrType                              24                 0.822199\n",
       "MasVnrArea                              23                 0.787941\n",
       "MSZoning                                 4                 0.137033\n",
       "Functional                               2                 0.068517\n",
       "BsmtHalfBath                             2                 0.068517"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate about missing data\n",
    "# Analyze top 20 columns having missing data\n",
    "total = combined_df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (combined_df.isnull().sum()/combined_df.isnull().count()*100).sort_values(ascending=False)\n",
    "missing_data = pd.concat(\n",
    "    [total, percent], axis=1,\n",
    "    keys=[\"Total Number of Missing Data\",\"Missing Data Percentage\"])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='missing_data_handling'></a>\n",
    "## Missing Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We can safely drop columns with ~80% or more missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_will_be_removed = ['PoolQC','MiscFeature','Alley','Fence']\n",
    "\n",
    "combined_df.drop(columns_will_be_removed, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "# Double check whether above columns are removed\n",
    "total = combined_df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (combined_df.isnull().sum() / combined_df.isnull().count() * 100).sort_values(ascending=False)\n",
    "\n",
    "def assert_column_drop(data, col_names):\n",
    "    for col_name in col_names:\n",
    "        assert col_name not in data, \"%s should not exist\" % col_name\n",
    "        \n",
    "assert_column_drop(combined_df, columns_will_be_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data set after removing columns with 80% missing data: (2919, 70)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of data set after removing columns with 80% missing data: {}\".format(combined_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='impute_NaN_values'></a>\n",
    "## Missing Values Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For imputing missing values, we looked at each column and its description (in the given description text file) independently and chose imputed values individually. For some columns, default value is given in the description file.\n",
    "1. From documentation: data_description.txt, the following column's default value (when NaN) is **None**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_none_columns = ['FireplaceQu','GarageType','GarageFinish','GarageQual',\n",
    "                        'GarageCond','BsmtQual','BsmtCond','BsmtExposure',\n",
    "                        'BsmtFinType1','BsmtFinType2','MasVnrType']\n",
    "\n",
    "for none_column in default_none_columns:\n",
    "    combined_df[none_column].fillna('None', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. From documentation: data_description.txt, the following column's default value (when NaN) is **0**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_zero_columns = ['GarageYrBlt','GarageArea','BsmtFinSF1','BsmtFinSF2',\n",
    "                        'BsmtUnfSF','BsmtFullBath','BsmtHalfBath','MasVnrArea']\n",
    "\n",
    "for zero_column in default_zero_columns:\n",
    "    combined_df[zero_column].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Using background knowledge, we set the missing value to the **mode** for some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Electrical'].fillna(combined_df['Electrical'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. *LotFrontage*: We will replace missing values with **mean** of the available data points for this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['LotFrontage'].fillna(combined_df['LotFrontage'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We observed that values for some rows in the **test** data set has missing values (these columns do not have missing values in the train dataset). Missing values for these Categorical (Ordinal) type features: we will impute them with **mode** of the available values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['MSZoning'].fillna(combined_df['MSZoning'].mode()[0], inplace=True)\n",
    "combined_df['Functional'].fillna(combined_df['Functional'].mode()[0], inplace=True)\n",
    "combined_df['SaleType'].fillna(combined_df['SaleType'].mode()[0], inplace=True)\n",
    "combined_df['KitchenQual'].fillna(combined_df['KitchenQual'].mode()[0], inplace=True)\n",
    "combined_df['Exterior1st'].fillna(combined_df['Exterior1st'].mode()[0], inplace=True)\n",
    "combined_df['Exterior2nd'].fillna(combined_df['Exterior2nd'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that there are no more missing values in the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert combined_df.isnull().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='add_new_cols'></a>\n",
    "## Adding New Columns (Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Informed by our background understanding of the domain, ee are adding the following two features (as a combination of other existing features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['TotalAreaSF'] = combined_df['GrLivArea'] + combined_df['GarageArea'] + combined_df['EnclosedPorch'] + combined_df['ScreenPorch']\n",
    "combined_df['LivingAreaSF'] = combined_df['1stFlrSF'] + combined_df['2ndFlrSF']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_preparation'></a>\n",
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will prepare data for machine learning modeling. This includes, preprocessing, transformations, encoding and scaling data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='log_transformations'></a>\n",
    "### Log Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we found during univariate analysis, log transformation of the output variable *SalePrice* would help with modeling linear regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_all = np.log(Y_train_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also perform log transformation for these numeric features. Note that since the min value of *PoolArea* is 0, we cannot perform log transformation. We will, instead, use *log1p* transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_features = ['LotArea','PoolArea']\n",
    "\n",
    "for log_feature in log_features:\n",
    "    combined_df[log_feature] = np.log1p(combined_df[log_feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ordinal_encoding'></a>\n",
    "### Manually Encoding Ordinal Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We manually encode ordered values for ordinal categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['BsmtQual'] = combined_df['BsmtQual'].map({\"None\":0, \"Fa\":1, \"TA\":2, \"Gd\":3, \"Ex\":4})\n",
    "combined_df['BsmtCond'] = combined_df['BsmtCond'].map({\"None\":0, \"Po\":1, \"Fa\":2, \"TA\":3, \"Gd\":4, \"Ex\":5})\n",
    "combined_df['BsmtExposure'] = combined_df['BsmtExposure'].map({\"None\":0, \"NA\":0, \"No\":1, \"Mn\":2, \"Av\":3, \"Gd\":4})\n",
    "combined_df['KitchenQual'] = combined_df['KitchenQual'].map({\"Fa\":1, \"TA\":2, \"Gd\":3, \"Ex\":4})\n",
    "combined_df['ExterQual'] = combined_df['ExterQual'].map({\"Po\":0, \"Fa\":1, \"TA\":2, \"Gd\":3, \"Ex\":4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_binning'></a>\n",
    "### Data Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To reduce side impact of filled data with default value, we decided to bin year data into pre-1950, 1950-1974, 1975 -2000, and post-2000, respectively. By this approach, if default value is filled by zero, it will be categorized into pre-1950 instead of recognizing data itself as zero. We can assume that if data is missing, house data may not be recorded properly since it was built or remodeled before 1950.\n",
    "2. After binning, the target column having continuous integer value will be dropped.\n",
    "3. As a final step, value will be transformed into discrete value such as 1, 2, 3, or 4. Since recent update on the property can have a positive impact, larger value assigned for the recent years can be justfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GarageBlt column values: 1,2,3,4\n",
      "YrRemodeled column values: 2,3,4\n",
      "YrBuilt column values: 1,2,4\n"
     ]
    }
   ],
   "source": [
    "PERIOD_TO_VALUE = {'0': 0, 'Before1950': 1, '1950to1974': 2, '1975to1999': 3, '2000OrLater': 4}\n",
    "\n",
    "# Data Binning\n",
    "\n",
    "# GarageYrBlt -> GarageBlt\n",
    "combined_df.loc[(combined_df[\"GarageYrBlt\"].apply(int) < 1950) & (combined_df[\"GarageYrBlt\"].apply(int) >= 0), \"GarageBlt\"] = 'Before1950'\n",
    "combined_df.loc[(combined_df[\"GarageYrBlt\"].apply(int) < 1975) & (combined_df[\"GarageYrBlt\"].apply(int) >= 1950), \"GarageBlt\"] = '1950to1974'\n",
    "combined_df.loc[(combined_df[\"GarageYrBlt\"].apply(int) < 2000) & (combined_df[\"GarageYrBlt\"].apply(int) >= 1975), \"GarageBlt\"] = '1975to1999'\n",
    "combined_df.loc[combined_df[\"GarageYrBlt\"].apply(int) >= 2000, \"GarageBlt\"] = '2000OrLater'\n",
    "\n",
    "# YearRemodAdd -> YrRemodeled\n",
    "# Assume that there is no remodeling if built year == remodeled year\n",
    "combined_df.loc[combined_df[\"YearRemodAdd\"] == combined_df[\"YearBuilt\"], \"YrRemodeled\"] = 0\n",
    "combined_df.loc[(combined_df[\"YearRemodAdd\"] < 1950) & (combined_df[\"YearRemodAdd\"] != 0), \"YrRemodeled\"] = 'Before1950'\n",
    "combined_df.loc[(combined_df[\"YearRemodAdd\"] < 1975) & (combined_df[\"YearRemodAdd\"] >= 1950), \"YrRemodeled\"] = '1950to1974'\n",
    "combined_df.loc[(combined_df[\"YearRemodAdd\"] < 2000) & (combined_df[\"YearRemodAdd\"] >= 1975), \"YrRemodeled\"] = '1975to1999'\n",
    "combined_df.loc[combined_df[\"YearRemodAdd\"] >= 2000, \"YrRemodeled\"] = '2000OrLater'\n",
    "\n",
    "# YearBuilt -> YrBuilt\n",
    "combined_df.loc[combined_df[\"YearBuilt\"] < 1950, \"YrBuilt\"] = 'Before1950'\n",
    "combined_df.loc[(combined_df[\"YearBuilt\"] < 1975) & (combined_df[\"YearBuilt\"] >= 1950), \"YrBuilt\"] = '1950to1974'\n",
    "combined_df.loc[(combined_df[\"YearBuilt\"] < 2000) & (combined_df[\"YearBuilt\"] >= 1975), \"YrBuilt\"] = '1950to1974'\n",
    "combined_df.loc[combined_df[\"YearBuilt\"] >= 2000, \"YrBuilt\"] = '2000OrLater'\n",
    "\n",
    "combined_df['GarageBlt'] = combined_df['GarageBlt'].apply(lambda period: PERIOD_TO_VALUE[period])\n",
    "combined_df['YrRemodeled'] = combined_df['YrRemodeled'].apply(lambda period: PERIOD_TO_VALUE[period])\n",
    "combined_df['YrBuilt'] = combined_df['YrBuilt'].apply(lambda period: PERIOD_TO_VALUE[period])\n",
    "\n",
    "def get_distinct_values(values):\n",
    "    return \",\".join([str(i) for i in (set(values))])\n",
    "\n",
    "# Check data binning results\n",
    "print(\"GarageBlt column values:\", get_distinct_values(combined_df[\"GarageBlt\"]))\n",
    "print(\"YrRemodeled column values:\", get_distinct_values(combined_df[\"YrRemodeled\"]))\n",
    "print(\"YrBuilt column values:\", get_distinct_values(combined_df[\"YrBuilt\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='binning_cleanup'></a>\n",
    "### Cleanup For Binning Applied Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns after data binning. Continuous data is no longer meaningful.\n",
    "combined_df.drop(['GarageYrBlt', 'YearRemodAdd', 'YearBuilt'], axis=1, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='categorical_encoding'></a>\n",
    "### Categorical Features: Conversion to Dummy/Indicator Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BldgType: 5, ['1Fam' '2fmCon' 'Duplex' 'TwnhsE' 'Twnhs']\n",
      "BsmtFinType1: 7, ['GLQ' 'ALQ' 'Unf' 'Rec' 'BLQ' 'None' 'LwQ']\n",
      "BsmtFinType2: 7, ['Unf' 'BLQ' 'None' 'ALQ' 'Rec' 'LwQ' 'GLQ']\n",
      "CentralAir: 2, ['Y' 'N']\n",
      "Condition1: 9, ['Norm' 'Feedr' 'PosN' 'Artery' 'RRAe' 'RRNn' 'RRAn' 'PosA' 'RRNe']\n",
      "Condition2: 8, ['Norm' 'Artery' 'RRNn' 'Feedr' 'PosN' 'PosA' 'RRAn' 'RRAe']\n",
      "Electrical: 5, ['SBrkr' 'FuseF' 'FuseA' 'FuseP' 'Mix']\n",
      "ExterCond: 5, ['TA' 'Gd' 'Fa' 'Po' 'Ex']\n",
      "FireplaceQu: 6, ['None' 'TA' 'Gd' 'Fa' 'Ex' 'Po']\n",
      "Foundation: 6, ['PConc' 'CBlock' 'BrkTil' 'Wood' 'Slab' 'Stone']\n",
      "Functional: 7, ['Typ' 'Min1' 'Maj1' 'Min2' 'Mod' 'Maj2' 'Sev']\n",
      "GarageCond: 6, ['TA' 'Fa' 'None' 'Gd' 'Po' 'Ex']\n",
      "GarageFinish: 4, ['RFn' 'Unf' 'Fin' 'None']\n",
      "GarageQual: 6, ['TA' 'Fa' 'Gd' 'None' 'Ex' 'Po']\n",
      "GarageType: 7, ['Attchd' 'Detchd' 'BuiltIn' 'CarPort' 'None' 'Basment' '2Types']\n",
      "Heating: 6, ['GasA' 'GasW' 'Grav' 'Wall' 'OthW' 'Floor']\n",
      "HeatingQC: 5, ['Ex' 'Gd' 'TA' 'Fa' 'Po']\n",
      "HouseStyle: 8, ['2Story' '1Story' '1.5Fin' '1.5Unf' 'SFoyer' 'SLvl' '2.5Unf' '2.5Fin']\n",
      "LandContour: 4, ['Lvl' 'Bnk' 'Low' 'HLS']\n",
      "LandSlope: 3, ['Gtl' 'Mod' 'Sev']\n",
      "LotConfig: 5, ['Inside' 'FR2' 'Corner' 'CulDSac' 'FR3']\n",
      "LotShape: 4, ['Reg' 'IR1' 'IR2' 'IR3']\n",
      "MSZoning: 5, ['RL' 'RM' 'C (all)' 'FV' 'RH']\n",
      "MasVnrType: 4, ['BrkFace' 'None' 'Stone' 'BrkCmn']\n",
      "PavedDrive: 3, ['Y' 'N' 'P']\n",
      "RoofMatl: 8, ['CompShg' 'WdShngl' 'Metal' 'WdShake' 'Membran' 'Tar&Grv' 'Roll'\n",
      " 'ClyTile']\n",
      "RoofStyle: 6, ['Gable' 'Hip' 'Gambrel' 'Mansard' 'Flat' 'Shed']\n",
      "SaleCondition: 6, ['Normal' 'Abnorml' 'Partial' 'AdjLand' 'Alloca' 'Family']\n",
      "SaleType: 9, ['WD' 'New' 'COD' 'ConLD' 'ConLI' 'CWD' 'ConLw' 'Con' 'Oth']\n"
     ]
    }
   ],
   "source": [
    "combined_df.shape\n",
    "category_columns = [f for f in combined_df.columns if combined_df.dtypes[f] == 'object']\n",
    "category_columns.sort()\n",
    "for col in category_columns:\n",
    "    if (train[col].nunique() <= 10):\n",
    "        print ('{}: {}, {}'.format(col, combined_df[col].nunique(), combined_df[col].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 257)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = pd.get_dummies(combined_df, drop_first=True)\n",
    "combined_df['MSSubClass'] = combined_df['MSSubClass'].astype(str)\n",
    "combined_df = pd.get_dummies(combined_df, columns = [\"MoSold\",'YrSold','MSSubClass'], drop_first=True)\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='split_back'></a>\n",
    "## Split Back into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1460, 257), (1459, 257))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_all = combined_df[:train.shape[0]]\n",
    "X_test = combined_df[train.shape[0]:]\n",
    "X_train_all.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modeling'></a>\n",
    "# 6. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll perform the following:\n",
    "* Preprocessing data to make it ready for machine learning modeling\n",
    "* Building and tuning home prices prediction models using cross-validations\n",
    "* Selecting the best model\n",
    " \n",
    "\n",
    "We plan to start with LinearRegression model because of the predictive nature of the problem. We will also try other supervised learning models such as Random Forest,Gradient Boosting Tree and if they increase accuracy.\n",
    "\n",
    "We will be working on two broad sets of algorithms:\n",
    "1. Linear Models\n",
    "2. Non Linear relationships using Random Forests\n",
    "\n",
    "We begin with (multiple) LinearRegression model (for speed) and if the accuracy is not satisfactory, we try other models such as random forest and gradient-boosting tree. We are ensembling all the models for better overall accuracy. We will split test data into two random sets and use one as \"dev\" data for measuring the performance during model building process.\n",
    "\n",
    "### Linear Models\n",
    "For linear models, we have regular OLS model, and the regularized linear models of Ridge Regression, Least Absolute Shrinkage and Selection Operator (LASSO), and Elastic Net. \n",
    "\n",
    "For model tuning, Sklearn's grid search with CV function is used to find the optimal hyper-parameter values.\n",
    "\n",
    "To assess the predictive performance of regression models, we can compute the mean sum of squared errors and the related summary metric. Furthermore, we also use graphical approach of residual plots to diagnose problems of linear regression models.\n",
    "\n",
    "We apply regularization to our regression models to reduce the model complexity and avoid overfitting.\n",
    "\n",
    "### Non Linear relationships using Random Forests\n",
    "Since random forests are less sensitive to outliers in the dataset we are assuming at this point that it will not require much parameter tuning. The only parameter that will require experimenting might be number of trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following hyperlinks direct to relevant subsections:\n",
    "- [Scaling Features](#scaled_feats)\n",
    "- [Create Cross Validation (Dev) Data Set for Testing Model Performance](#cross_validation)\n",
    "- [Using Grid Search to Tune Hyperparameters](#gridsearch_tuning)\n",
    "- [Predict Test and Prepare Submission](#predict_test_prep_submission)\n",
    "    - [Using the best Lasso model](#best_lasso)\n",
    "    - [Using the best Gradient Boosting model](#best_gb)\n",
    "    - [Using the ensemble of models](#ensemble)\n",
    "- [Interpreting Coefficients for Our Best Model - Lasso regularized](#lasso_regularized)\n",
    "- [Experiment on Clustering](#experiment_on_clustering)\n",
    "- [Drivers of Sale Price](#drivers_of_saleprice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scaled_feats'></a>\n",
    "## Scaling Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will scale all the input features before running models on them to transform them into standard Gaussian distribution. Some machine learning algorithms work better with this transformation (a feature with large scale does not dominate another feature with smaller scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_all)\n",
    "\n",
    "X_train_all = scaler.transform(X_train_all)\n",
    "X_train_all = pd.DataFrame(X_train_all, columns = combined_df.columns)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns = combined_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cross_validation'></a>\n",
    "## Create Cross Validation (Dev) Data Set for Testing Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin with dividing train data into train and dev for model benchmarking and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((292, 257), (292,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define cutoff to bisect train and cross validation data\n",
    "cutoff = (len(train) * 80) // 100\n",
    "\n",
    "X_dev = X_train_all[cutoff:]\n",
    "Y_dev = Y_train_all[cutoff:]\n",
    "X_train = X_train_all[:cutoff]\n",
    "Y_train = Y_train_all[:cutoff]\n",
    "X_dev.shape,Y_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predictions, targets):\n",
    "    return '%.4f' % np.sqrt(mean_squared_error(predictions,targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = defaultdict(dict)\n",
    "\n",
    "'''\n",
    "Run the given model and store benchmarks for later analysis.\n",
    "'''\n",
    "def benchmark(model, name=None):\n",
    "    if not name:\n",
    "        name = model.__class__.__name__\n",
    "    t0 = time.clock()\n",
    "    model.fit(X_train, Y_train)\n",
    "    res[name]['train_time'] = time.clock() - t0\n",
    "    res[name]['train_rmse'] = rmse(Y_train, model.predict(X_train))\n",
    "    \n",
    "    t0 = time.clock()\n",
    "    Y_pred = model.predict(X_dev)\n",
    "    res[name]['test_time'] = time.clock() - t0\n",
    "    res[name]['test_rmse'] = rmse(Y_dev, Y_pred)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run through the selected models for baseline before we tune them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_time</th>\n",
       "      <th>test_time</th>\n",
       "      <th>train_rmse</th>\n",
       "      <th>test_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>0.030685</td>\n",
       "      <td>0.000369778</td>\n",
       "      <td>21260.3931</td>\n",
       "      <td>29782090623064724.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.00609935</td>\n",
       "      <td>0.000333369</td>\n",
       "      <td>21274.3286</td>\n",
       "      <td>47145.9401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNet</th>\n",
       "      <td>0.031853</td>\n",
       "      <td>0.000349298</td>\n",
       "      <td>24178.2224</td>\n",
       "      <td>42927.2281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>0.466356</td>\n",
       "      <td>0.000628907</td>\n",
       "      <td>13711.4610</td>\n",
       "      <td>27299.9962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>0.145121</td>\n",
       "      <td>0.000820623</td>\n",
       "      <td>13192.3776</td>\n",
       "      <td>32273.8302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           train_time    test_time  train_rmse  \\\n",
       "LinearRegression             0.030685  0.000369778  21260.3931   \n",
       "Ridge                      0.00609935  0.000333369  21274.3286   \n",
       "ElasticNet                   0.031853  0.000349298  24178.2224   \n",
       "GradientBoostingRegressor    0.466356  0.000628907  13711.4610   \n",
       "RandomForestRegressor        0.145121  0.000820623  13192.3776   \n",
       "\n",
       "                                        test_rmse  \n",
       "LinearRegression           29782090623064724.0000  \n",
       "Ridge                                  47145.9401  \n",
       "ElasticNet                             42927.2281  \n",
       "GradientBoostingRegressor              27299.9962  \n",
       "RandomForestRegressor                  32273.8302  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running all models with default values\n",
    "benchmark(LinearRegression())\n",
    "benchmark(Ridge(alpha=1.0))\n",
    "benchmark(ElasticNet(alpha=1.0))\n",
    "est = benchmark(GradientBoostingRegressor(learning_rate=0.1, n_estimators=100))\n",
    "benchmark(RandomForestRegressor(n_estimators=10, max_depth=None))\n",
    "res_df = pd.DataFrame(data=res).T\n",
    "res_df[['train_time','test_time','train_rmse','test_rmse']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gridsearch_tuning'></a>\n",
    "## Using Grid Search to Tune Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression\n",
    "ALPHAS = np.array([10, 5, 0.1, 0.01, 0.001, 0.0001, 0])\n",
    "\n",
    "# Create and fit a ridge regression model, testing each alpha\n",
    "model = Ridge()\n",
    "grid = GridSearchCV(estimator=model, cv=10, param_grid=dict(alpha=ALPHAS), scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid.fit(X_train, Y_train)\n",
    "\n",
    "# Print best params and score\n",
    "print('Best parameters:', grid.best_estimator_.alpha)\n",
    "print('Best score:', -grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso regression\n",
    "ALPHAS = [0.00001, 0.0001, 0.001, 0.005, 0.01, 0.05]\n",
    "\n",
    "# Create and fit a Lasso regression model, testing each alpha\n",
    "ls = Lasso()\n",
    "grid_ls = GridSearchCV(estimator=ls, param_grid=dict(alpha=ALPHAS), scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_ls.fit(X_train, Y_train)\n",
    "\n",
    "# Print best params and score\n",
    "print('Best parameters:', grid_ls.best_estimator_.alpha)\n",
    "print('Best score:', -grid_ls.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic net regression\n",
    "param_grid = [{'alpha': np.logspace(0, 1, 10), 'l1_ratio': np.linspace(0, 1, 10)}]\n",
    "\n",
    "# Create and fit a Elastic net regression model\n",
    "grid_en = GridSearchCV(estimator=ElasticNet(), param_grid=param_grid, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_en.fit(X_train, Y_train)\n",
    "\n",
    "# Print best params and score\n",
    "print('Best parameters: alpha: {}, l1_ratio: {}'.format(grid_en.best_estimator_.alpha,grid_en.best_estimator_.l1_ratio))\n",
    "print('Best score: {}'.format(-grid_en.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_en.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting model\n",
    "param_grid={'n_estimators':[100],\n",
    "           'learning_rate': [0.01,0.1, 1],\n",
    "           'max_depth':[4,6,10],\n",
    "           'min_samples_leaf':[3,5]\n",
    "           }\n",
    "gbModel = GridSearchCV(estimator=GradientBoostingRegressor(), cv=5, param_grid=param_grid, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "gbModel.fit(X_train, Y_train)\n",
    "print('Best parameters: n_estimators: {}, learning_rate: {}, max_depth: {}, min_samples_leaf: {}'.format(\n",
    "    gbModel.best_estimator_.n_estimators,gbModel.best_estimator_.learning_rate, gbModel.best_estimator_.max_depth, gbModel.best_estimator_.min_samples_leaf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbModel.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest model\n",
    "param_grid={'n_estimators':[50,100],\n",
    "           'max_depth':[5,10,20],\n",
    "           }\n",
    "rfModel = GridSearchCV(estimator=RandomForestRegressor(), cv=5, param_grid=param_grid, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "rfModel.fit(X_train, Y_train)\n",
    "print('Best estimators:', rfModel.best_estimator_.n_estimators)\n",
    "print('Best max depth:', rfModel.best_estimator_.max_depth)\n",
    "print('Best score:', rfModel.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfModel.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_ls.best_estimator_.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using best parameters from Grid Search CV to find the best model\n",
    "benchmark(LinearRegression())\n",
    "benchmark(Lasso(alpha=grid_ls.best_estimator_.alpha))\n",
    "benchmark(Ridge(alpha=grid.best_estimator_.alpha))\n",
    "benchmark(ElasticNet(alpha=grid_en.best_estimator_.alpha, l1_ratio=grid_en.best_estimator_.l1_ratio))\n",
    "est = benchmark(GradientBoostingRegressor(max_depth=gbModel.best_estimator_.max_depth, min_samples_leaf=gbModel.best_estimator_.min_samples_leaf, learning_rate=gbModel.best_estimator_.learning_rate, n_estimators=100))\n",
    "benchmark(RandomForestRegressor(n_estimators=rfModel.best_estimator_.n_estimators, max_depth=rfModel.best_estimator_.max_depth))\n",
    "\n",
    "res_df = pd.DataFrame(data=res).T\n",
    "\n",
    "res_df[['train_time', 'test_time', 'train_rmse', 'test_rmse']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above results, we see that Lasso and GradientBoostingRegressor are the models with best results (lowest rmse on dev data). LinearRegression output is too off the chart primarily because of multicollinearlity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='predict_test_prep_submission'></a>\n",
    "## Predict Test and Prepare Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='best_lasso'></a>\n",
    "### Using the best Lasso model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Lasso as our 1st best model based on the above Grid Search results RMSE\n",
    "final_model = Lasso(alpha=grid_ls.best_estimator_.alpha)\n",
    "final_model.fit(X_train_all, Y_train_all)\n",
    "y_test = np.exp(final_model.predict(X_test))\n",
    "pred_df = pd.DataFrame()\n",
    "pred_df['Id'] = test_id\n",
    "pred_df['SalePrice'] = y_test\n",
    "pred_df.to_csv('w207_mlairev5_Lasso.csv', index=False)\n",
    "print('result csv generated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='best_gb'></a>\n",
    "### Using the best Gradient Boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gbr = GradientBoostingRegressor(max_depth=gbModel.best_estimator_.max_depth, min_samples_leaf=gbModel.best_estimator_.min_samples_leaf, learning_rate=gbModel.best_estimator_.learning_rate, n_estimators=100)\n",
    "final_gbr.fit(X_train_all, Y_train_all)\n",
    "y_test = np.exp(final_gbr.predict(X_test))\n",
    "pred_df = pd.DataFrame()\n",
    "pred_df['Id'] = test_id\n",
    "pred_df['SalePrice'] = y_test\n",
    "pred_df.to_csv('w207_mlairev3.csv', index=False)\n",
    "print('result csv generated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Analyzing our 2nd best model GradientBoostingRegressor by getting relative importance of each attribute\n",
    "varImp = pd.DataFrame({'index':X_train_all.columns, 'feature_importance':gbModel.best_estimator_.feature_importances_})\n",
    "varImp.sort_values(by='feature_importance', ascending=False, inplace=True)\n",
    "f, ax = plt.subplots(1, 1, figsize=[12, 9])\n",
    "\n",
    "p = sns.barplot(x = 'feature_importance', y = 'index', data = varImp.iloc[:30,], ax=ax);\n",
    "p.set_ylabel(\"Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='kaggle_submissions_gbr.png', width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ensemble'></a>\n",
    "### Using the ensemble of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will ensemble(combine) results from all models (except LinearRegression model because that is way off) to prepare an ensemble prediction. The idea behind trying this approach is that some models may be good in explaning a particular type of variability (or signal). Combining these weak models will, hoepefully, help us pick the best of these individual models.\n",
    "\n",
    "Since the performance of all 5 models are pretty close, we will use simple mean to produce ensemble predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensemble\n",
    "final_ridge = Ridge(alpha=grid.best_estimator_.alpha)\n",
    "final_elasticnet = ElasticNet(alpha=grid_en.best_estimator_.alpha, l1_ratio=grid_en.best_estimator_.l1_ratio)\n",
    "final_randomforest = RandomForestRegressor(n_estimators=rfModel.best_estimator_.n_estimators, max_depth=rfModel.best_estimator_.max_depth)\n",
    "\n",
    "models = [final_model, final_ridge, final_elasticnet, final_gbr, final_randomforest]\n",
    "model_names = ['Lasso', 'Ridge', 'ElasticNet', 'GradientBoosting', 'RandomForest']\n",
    "final_predictions = pd.DataFrame()\n",
    "\n",
    "row_index = 0\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train_all, Y_train_all)\n",
    "    final_predictions.insert(loc=row_index, column=model_names[0], value=np.exp(model.predict(X_test)))    \n",
    "    model_names.pop(0)\n",
    "    row_index += 1\n",
    "\n",
    "final_predictions['mean_prediction'] = final_predictions.mean(axis=1)\n",
    "print(\"Sample predictions...\")    \n",
    "final_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_ens = pd.DataFrame()\n",
    "pred_df_ens['Id'] = test_id\n",
    "pred_df_ens['SalePrice'] = final_predictions['mean_prediction']\n",
    "pred_df_ens.to_csv('w207_mlaire_ensemble_v1.csv',index=False)\n",
    "print('ensemble result csv generated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lasso_regularized'></a>\n",
    "## Interpreting Coefficients for Our Best Model - Lasso regularized "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our best model Lasso will be plot top 10 coefficients using the below function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coefficients(final_model, n = 10):\n",
    "    cols = X_train_all.columns\n",
    "    coeffs = final_model.coef_\n",
    "    zipped = list(zip(cols, coeffs))\n",
    "    zipped.sort(key=lambda x: x[1], reverse=True)\n",
    "    postive_10 = pd.DataFrame(zipped).head(n)\n",
    "    negative_10 = pd.DataFrame(zipped).tail(n)\n",
    "    return pd.concat([postive_10, negative_10], axis=0).plot.barh(x=0, y=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coefficients(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"% of Features used in the best model: \" + str(100 *(sum(final_model.coef_ > 0) / final_model.coef_.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='drivers_of_saleprice'></a>\n",
    "## Drivers of Sale Price "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above coefficients plot, we see that the feature that adds the most value to the Sale Price of a home in Ames, Iowa is the 'TotalAreaSF', which is combination of GrLivArea,GarageArea, EnclosedPorch and ScreenPorch areas. Other features that have a strong postive effect on the Sale Price are OverallQual that rates the overall material and finish of the house, Living area in square feet ,LotArea: Lot size in square feet and the rating for overall condition of the house.\n",
    "\n",
    "On the other hand, we see that the Neighborhood_IDOTRR has most negative impact on the Sale Price. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "# 7. Summary & Key Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will present our key findings in terms of key predictor variables and their parameter values. We will also summarize the modeling process and learning from it. Feature engineering is one of the most critical part for final model performance. In future, We could try some of the interesting ideas summarized below  \n",
    "- Advanced transformations such as Box Cox and PCA  \n",
    "- Clustering analysisfor generating new categorical features  \n",
    "- Advanced modelling techniques with XGBoost, Stacking of Models etc  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle submissions and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='kaggle_submissions.png', width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='kaggle_rank.png', width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "# 8. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]: <a href=\"https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\">L1 and L2 Regulazation Model</a><br>\n",
    "[2]: <a href=\"https://chrisalbon.com/machine_learning/linear_regression/effect_of_alpha_on_lasso_regression/\">Effect of Alpha on Lasso Regression</a><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
